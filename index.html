<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"style":null,"show_result":false},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="YzRuntime&#39;s blog">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="YzRuntime&#39;s blog">
<meta property="og:locale" content="en_US">
<meta property="article:tag" content="Spark">
<meta property="article:tag" content="Big data">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>YzRuntime's blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">YzRuntime's blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="home fa-fw"></i>Home</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/06/Spring-Batch-4-10-ThottleLimit-Causing-Deadlock-on-ThreadPoolExecutor/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YzRuntime's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | YzRuntime's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/06/Spring-Batch-4-10-ThottleLimit-Causing-Deadlock-on-ThreadPoolExecutor/" class="post-title-link" itemprop="url">Spring Batch 5.1.0 ThottleLimit Causing Deadlock on ThreadPoolExecutor</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2025-07-06 19:10:58 / Modified: 19:16:34" itemprop="dateCreated datePublished" datetime="2025-07-06T19:10:58+08:00">2025-07-06</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Spring-Batch-4-10-ThottleLimit-Causing-Deadlock-on-ThreadPoolExecutor"><a href="#Spring-Batch-4-10-ThottleLimit-Causing-Deadlock-on-ThreadPoolExecutor" class="headerlink" title="Spring Batch 4.10 ThottleLimit Causing Deadlock on ThreadPoolExecutor"></a>Spring Batch 4.10 ThottleLimit Causing Deadlock on ThreadPoolExecutor</h1><h2 id="Facts"><a href="#Facts" class="headerlink" title="Facts"></a>Facts</h2><p>In Spring Batch 4.10,</p>
<ol>
<li>The ThottleLimit control roughtly how many task been submitted to the Executor in parallel, before waiting the result.</li>
<li>The ThottleLimit can be compromized, which means the actual submittion may more than ThottleLimit</li>
<li>When we use ThreadPoolExecutor + AbortPolicy (by default) and Spring Batch submit more task to trigger the ThreadPoolExecutor rejecting the task,<ol>
<li>Spring Batch won’t submit it again.</li>
<li>Except output an error log, Spring Batch won’t provide any feedback to user.</li>
<li>Spring Batch will still wait the task result from an failed submitted task, this can lock the whole process except the running subthread.</li>
</ol>
</li>
</ol>
<h2 id="Adviced-practice"><a href="#Adviced-practice" class="headerlink" title="Adviced practice"></a>Adviced practice</h2><ol>
<li>When using ThreadPoolExecutor + AbortPolicy (by default) ThrottleLimit need to be far less than ThreadPoolExecutor CorePoolSize.</li>
<li>When letting ThottleLimit &gt;= CorePoolSize, we have to use ThreadPoolExecutor’s CallerRunsPolicy to make sure the task submitting behaviour is backpressured, letting Spring Batch wait the correct number of the process. </li>
</ol>
<h2 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h2><p>We can use the Spring official example to reproduce following situations: </p>
<h3 id="1-Testing-DeadLock-when-Spring-Batch-submit-more-task-than-CorePoolSize-with-ThreadPoolExecutor-AbortPolicy"><a href="#1-Testing-DeadLock-when-Spring-Batch-submit-more-task-than-CorePoolSize-with-ThreadPoolExecutor-AbortPolicy" class="headerlink" title="1. Testing: DeadLock when Spring Batch submit more task than CorePoolSize, with ThreadPoolExecutor + AbortPolicy"></a>1. Testing: DeadLock when Spring Batch submit more task than CorePoolSize, with ThreadPoolExecutor + AbortPolicy</h3><h4 id="1-1-Source-code-branch"><a href="#1-1-Source-code-branch" class="headerlink" title="1.1 Source code branch"></a>1.1 Source code branch</h4><p><a target="_blank" rel="noopener" href="https://github.com/Solodye/spring-batch/tree/test/v5.1.0/deadlock-when-spring-batch-submit-more-task-than-core-pool-size">test/v5.1.0/deadlock-when-spring-batch-submit-more-task-than-core-pool-size</a>.</p>
<h4 id="1-2-Source-code-change"><a href="#1-2-Source-code-change" class="headerlink" title="1.2 Source code change"></a>1.2 Source code change</h4><p>Setting up the ThreadPoolExecutor like <a target="_blank" rel="noopener" href="https://github.com/spring-projects/spring-batch/commit/d6ce16cee1bc5a8d09a3a73f8023884edc77261b#diff-fd68889e30cb9c170db4da81237c216ff67b115edc4dcdc69ad464d2dff7d186R70-R79">this</a>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Bean</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ThreadPoolTaskExecutor <span class="title">threadPoolTaskExecutor</span><span class="params">()</span></span>&#123;</span><br><span class="line">    ThreadPoolTaskExecutor threadPoolTaskExecutor = <span class="keyword">new</span> ThreadPoolTaskExecutor();</span><br><span class="line">    threadPoolTaskExecutor.setCorePoolSize(<span class="number">2</span>);</span><br><span class="line">    threadPoolTaskExecutor.setMaxPoolSize(<span class="number">3</span>);</span><br><span class="line">    threadPoolTaskExecutor.setQueueCapacity(<span class="number">1</span>);</span><br><span class="line">    threadPoolTaskExecutor.setDaemon(<span class="keyword">true</span>);</span><br><span class="line">    threadPoolTaskExecutor.initialize();</span><br><span class="line">    <span class="keyword">return</span> threadPoolTaskExecutor;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>While we set <a target="_blank" rel="noopener" href="https://github.com/spring-projects/spring-batch/commit/d6ce16cee1bc5a8d09a3a73f8023884edc77261b#diff-fd68889e30cb9c170db4da81237c216ff67b115edc4dcdc69ad464d2dff7d186R64-R65">thottleLimit = 8 &gt; MaxPoolSize + QueueCapacity = 4</a></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.taskExecutor(threadPoolTaskExecutor())</span><br><span class="line">.throttleLimit(<span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<h4 id="1-3-Running-phenomenon"><a href="#1-3-Running-phenomenon" class="headerlink" title="1.3 Running phenomenon"></a>1.3 Running phenomenon</h4><p>It will output a the following log and <strong>stuck there</strong></p>
<details open>
  <summary>See log detail</summary>
  
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">2025-07-06 17:59:54 INFO  [main] BatchRegistrar:70 - Finished Spring Batch infrastructure beans configuration in 0 ms.</span><br><span class="line">2025-07-06 17:59:54 WARN  [main] PostProcessorRegistrationDelegate$BeanPostProcessorChecker:437 - Bean &#x27;jobRegistry&#x27; of type [org.springframework.batch.core.configuration.support.MapJobRegistry] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying). Is this bean getting eagerly injected into a currently created BeanPostProcessor [jobRegistryBeanPostProcessor]? Check the corresponding BeanPostProcessor declaration and its dependencies.</span><br><span class="line">2025-07-06 17:59:54 INFO  [main] EmbeddedDatabaseFactory:189 - Starting embedded database: url=&#x27;jdbc:hsqldb:mem:d99a2b92-7e2d-4b5d-87a8-30a2855a354b&#x27;, username=&#x27;sa&#x27;</span><br><span class="line">2025-07-06 17:59:54 INFO  [main] JobRepositoryFactoryBean:274 - No database type set, using meta data indicating: HSQL</span><br><span class="line">2025-07-06 17:59:54 INFO  [main] TaskExecutorRepeatTemplate:79 - Set throttleLimit to 8</span><br><span class="line">2025-07-06 17:59:54 INFO  [main] BatchObservabilityBeanPostProcessor:62 - No Micrometer observation registry found, defaulting to ObservationRegistry.NOOP</span><br><span class="line">2025-07-06 17:59:54 INFO  [main] SimpleJobLauncher:232 - No TaskExecutor has been set, defaulting to synchronous executor.</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] SimpleJobLauncher:154 - Job: [SimpleJob: [name=ioSampleJob]] launched with the following parameters: [&#123;&#x27;inputFile&#x27;:&#x27;&#123;value=org/springframework/batch/samples/file/delimited/data/delimited.csv, type=class java.lang.String, identifying=true&#125;&#x27;,&#x27;outputFile&#x27;:&#x27;&#123;value=file:./target/test-outputs/delimitedOutput.csv, type=class java.lang.String, identifying=true&#125;&#x27;&#125;]</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] SimpleStepHandler:150 - Executing step: [step1]</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] TaskExecutorRepeatTemplate:133 - Entering RepeatStatus.iterate</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] TaskExecutorRepeatTemplate:161 - RepeatStatus.executeInternal</span><br><span class="line">Thread main is setting throtleLimit 8 to ResultHolderResultQueue</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] TaskExecutorRepeatTemplate:127 - Submitting jobs org.springframework.batch.core.step.tasklet.TaskletStep$2@2ee39e73</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] TaskExecutorRepeatTemplate:129 - Done submitting jobs org.springframework.batch.core.step.tasklet.TaskletStep$2@2ee39e73</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] TaskExecutorRepeatTemplate:127 - Submitting jobs org.springframework.batch.core.step.tasklet.TaskletStep$2@2ee39e73</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-1] TaskExecutorRepeatTemplate:259 - run method start</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] TaskExecutorRepeatTemplate:129 - Done submitting jobs org.springframework.batch.core.step.tasklet.TaskletStep$2@2ee39e73</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] TaskExecutorRepeatTemplate:127 - Submitting jobs org.springframework.batch.core.step.tasklet.TaskletStep$2@2ee39e73</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-2] TaskExecutorRepeatTemplate:259 - run method start</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] TaskExecutorRepeatTemplate:129 - Done submitting jobs org.springframework.batch.core.step.tasklet.TaskletStep$2@2ee39e73</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] TaskExecutorRepeatTemplate:127 - Submitting jobs org.springframework.batch.core.step.tasklet.TaskletStep$2@2ee39e73</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] TaskExecutorRepeatTemplate:129 - Done submitting jobs org.springframework.batch.core.step.tasklet.TaskletStep$2@2ee39e73</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] TaskExecutorRepeatTemplate:127 - Submitting jobs org.springframework.batch.core.step.tasklet.TaskletStep$2@2ee39e73</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-3] TaskExecutorRepeatTemplate:259 - run method start</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] TaskExecutorRepeatTemplate:298 - Handling exception: org.springframework.core.task.TaskRejectedException, caused by: org.springframework.core.task.TaskRejectedException: ExecutorService in active state did not accept task: org.springframework.batch.repeat.support.TaskExecutorRepeatTemplate$ExecutingRunnable@c4d2c44</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] TaskExecutorRepeatTemplate:224 - result is CONTINUABLE</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] TaskExecutorRepeatTemplate:162 - Entering waitForResults</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] TaskExecutorRepeatTemplate:176 - Running queue.take()</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-1] RepeatTemplate:133 - Entering RepeatStatus.iterate</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-2] RepeatTemplate:133 - Entering RepeatStatus.iterate</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-3] RepeatTemplate:133 - Entering RepeatStatus.iterate</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-3] RepeatTemplate:161 - RepeatStatus.executeInternal</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-2] RepeatTemplate:161 - RepeatStatus.executeInternal</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-1] RepeatTemplate:161 - RepeatStatus.executeInternal</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-1] FlatFileItemReader:188 - Reading customer3,30</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-2] FlatFileItemReader:188 - Reading customer2,20</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-3] FlatFileItemReader:188 - Reading customer1,10</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-2] FlatFileItemReader:188 - Reading customer6,60</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-3] FlatFileItemReader:188 - Reading customer5,50</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-1] FlatFileItemReader:188 - Reading customer4,40</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-2] RepeatTemplate:224 - result is CONTINUABLE</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-1] RepeatTemplate:224 - result is CONTINUABLE</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-3] RepeatTemplate:224 - result is CONTINUABLE</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-1] RepeatTemplate:383 - Entering no-op waitForResults</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-2] RepeatTemplate:383 - Entering no-op waitForResults</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-3] RepeatTemplate:383 - Entering no-op waitForResults</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-1] RepeatTemplate:229 - Get result CONTINUABLE</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-3] RepeatTemplate:229 - Get result CONTINUABLE</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-2] RepeatTemplate:229 - Get result CONTINUABLE</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-1] RepeatTemplate:149 - Quit RepeatStatus.iterate</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-3] RepeatTemplate:149 - Quit RepeatStatus.iterate</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-2] RepeatTemplate:149 - Quit RepeatStatus.iterate</span><br><span class="line">2025-07-06 17:59:55 WARN  [threadPoolTaskExecutor-3] TaskExecutorRepeatTemplate:276 - run method got error Conversion = &#x27; &#x27;</span><br><span class="line">2025-07-06 17:59:55 WARN  [threadPoolTaskExecutor-2] TaskExecutorRepeatTemplate:276 - run method got error Conversion = &#x27; &#x27;</span><br><span class="line">2025-07-06 17:59:55 WARN  [threadPoolTaskExecutor-1] TaskExecutorRepeatTemplate:276 - run method got error Conversion = &#x27; &#x27;</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-2] ResultHolderResultQueue:99 - Put this in queue org.springframework.batch.repeat.support.TaskExecutorRepeatTemplate$ExecutingRunnable@2c6cfb64</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-3] ResultHolderResultQueue:99 - Put this in queue org.springframework.batch.repeat.support.TaskExecutorRepeatTemplate$ExecutingRunnable@6802dc5f</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-1] ResultHolderResultQueue:99 - Put this in queue org.springframework.batch.repeat.support.TaskExecutorRepeatTemplate$ExecutingRunnable@7e8cc5f8</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] ResultHolderResultQueue:138 - Queue size before take: 1</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] ResultHolderResultQueue:139 - Take value: isContinuable = false</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] ResultHolderResultQueue:140 - Queue size after take: 2</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-1] ResultHolderResultQueue:105 - Notify all</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-1] TaskExecutorRepeatTemplate:285 - run method end finally</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-3] ResultHolderResultQueue:105 - Notify all</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-3] TaskExecutorRepeatTemplate:285 - run method end finally</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-1] TaskExecutorRepeatTemplate:259 - run method start</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-2] ResultHolderResultQueue:105 - Notify all</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-2] TaskExecutorRepeatTemplate:285 - run method end finally</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] ResultHolderResultQueue:150 - Enter in waiting, count = 5 , queue.size() = 3</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-1] TaskExecutorRepeatTemplate:272 - run method end try</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-1] ResultHolderResultQueue:99 - Put this in queue org.springframework.batch.repeat.support.TaskExecutorRepeatTemplate$ExecutingRunnable@27a5151b</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-1] ResultHolderResultQueue:105 - Notify all</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-1] TaskExecutorRepeatTemplate:285 - run method end finally</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] ResultHolderResultQueue:154 - Wake up, count = 5 , queue.size() = 4</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] ResultHolderResultQueue:150 - Enter in waiting, count = 5 , queue.size() = 4</span><br></pre></td></tr></table></figure>
</details>

<details>
 <summary>Another log detail</summary>

 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"> 2025-07-06 19:03:56 INFO  [main] BatchRegistrar:70 - Finished Spring Batch infrastructure beans configuration in 0 ms.</span><br><span class="line">2025-07-06 19:03:56 WARN  [main] PostProcessorRegistrationDelegate$BeanPostProcessorChecker:437 - Bean &#x27;jobRegistry&#x27; of type [org.springframework.batch.core.configuration.support.MapJobRegistry] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying). Is this bean getting eagerly injected into a currently created BeanPostProcessor [jobRegistryBeanPostProcessor]? Check the corresponding BeanPostProcessor declaration and its dependencies.</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] EmbeddedDatabaseFactory:189 - Starting embedded database: url=&#x27;jdbc:hsqldb:mem:51832490-316a-4a89-b4cc-57683b46885a&#x27;, username=&#x27;sa&#x27;</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] JobRepositoryFactoryBean:274 - No database type set, using meta data indicating: HSQL</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:79 - Set throttleLimit to 8</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] BatchObservabilityBeanPostProcessor:62 - No Micrometer observation registry found, defaulting to ObservationRegistry.NOOP</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] SimpleJobLauncher:232 - No TaskExecutor has been set, defaulting to synchronous executor.</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] SimpleJobLauncher:154 - Job: [SimpleJob: [name=ioSampleJob]] launched with the following parameters: [&#123;&#x27;inputFile&#x27;:&#x27;&#123;value=org/springframework/batch/samples/file/delimited/data/delimited.csv, type=class java.lang.String, identifying=true&#125;&#x27;,&#x27;outputFile&#x27;:&#x27;&#123;value=file:./target/test-outputs/delimitedOutput.csv, type=class java.lang.String, identifying=true&#125;&#x27;&#125;]</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] SimpleStepHandler:150 - Executing step: [step1]</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:133 - Entering RepeatStatus.iterate</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:161 - RepeatStatus.executeInternal</span><br><span class="line">Thread main is setting throtleLimit 8 to ResultHolderResultQueue</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:127 - Submitting jobs org.springframework.batch.core.step.tasklet.TaskletStep$2@2ee39e73</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:129 - Done submitting jobs org.springframework.batch.core.step.tasklet.TaskletStep$2@2ee39e73</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:127 - Submitting jobs org.springframework.batch.core.step.tasklet.TaskletStep$2@2ee39e73</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-1] TaskExecutorRepeatTemplate:259 - run method start</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:129 - Done submitting jobs org.springframework.batch.core.step.tasklet.TaskletStep$2@2ee39e73</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:127 - Submitting jobs org.springframework.batch.core.step.tasklet.TaskletStep$2@2ee39e73</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-2] TaskExecutorRepeatTemplate:259 - run method start</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:129 - Done submitting jobs org.springframework.batch.core.step.tasklet.TaskletStep$2@2ee39e73</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:127 - Submitting jobs org.springframework.batch.core.step.tasklet.TaskletStep$2@2ee39e73</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:129 - Done submitting jobs org.springframework.batch.core.step.tasklet.TaskletStep$2@2ee39e73</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:127 - Submitting jobs org.springframework.batch.core.step.tasklet.TaskletStep$2@2ee39e73</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] TaskExecutorRepeatTemplate:259 - run method start</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:298 - Handling exception: org.springframework.core.task.TaskRejectedException, caused by: org.springframework.core.task.TaskRejectedException: ExecutorService in active state did not accept task: org.springframework.batch.repeat.support.TaskExecutorRepeatTemplate$ExecutingRunnable@c4d2c44</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:224 - result is CONTINUABLE</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:162 - Entering waitForResults</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:176 - Running queue.take()</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:137 - Queue size before take: 0</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-2] RepeatTemplate:133 - Entering RepeatStatus.iterate</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] RepeatTemplate:133 - Entering RepeatStatus.iterate</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-1] RepeatTemplate:133 - Entering RepeatStatus.iterate</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-2] RepeatTemplate:161 - RepeatStatus.executeInternal</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] RepeatTemplate:161 - RepeatStatus.executeInternal</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-1] RepeatTemplate:161 - RepeatStatus.executeInternal</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-2] FlatFileItemReader:188 - Reading customer1,10</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] FlatFileItemReader:188 - Reading customer3,30</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-1] FlatFileItemReader:188 - Reading customer2,20</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] FlatFileItemReader:188 - Reading customer5,50</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-2] FlatFileItemReader:188 - Reading customer4,40</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-1] FlatFileItemReader:188 - Reading customer6,60</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-1] RepeatTemplate:224 - result is CONTINUABLE</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-2] RepeatTemplate:224 - result is CONTINUABLE</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] RepeatTemplate:224 - result is CONTINUABLE</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-1] RepeatTemplate:383 - Entering no-op waitForResults</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-2] RepeatTemplate:383 - Entering no-op waitForResults</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] RepeatTemplate:383 - Entering no-op waitForResults</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-1] RepeatTemplate:229 - Get result CONTINUABLE</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-2] RepeatTemplate:229 - Get result CONTINUABLE</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] RepeatTemplate:229 - Get result CONTINUABLE</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-2] RepeatTemplate:149 - Quit RepeatStatus.iterate</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-1] RepeatTemplate:149 - Quit RepeatStatus.iterate</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] RepeatTemplate:149 - Quit RepeatStatus.iterate</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-2] AbstractFileItemWriter:76 - Writing CustomerCredit [id=0,name=customer1, credit=15] </span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-1] AbstractFileItemWriter:76 - Writing CustomerCredit [id=0,name=customer2, credit=25] </span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] AbstractFileItemWriter:76 - Writing CustomerCredit [id=0,name=customer3, credit=35] </span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] AbstractFileItemWriter:76 - Writing CustomerCredit [id=0,name=customer5, credit=55] </span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-1] AbstractFileItemWriter:76 - Writing CustomerCredit [id=0,name=customer6, credit=65] </span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-2] AbstractFileItemWriter:76 - Writing CustomerCredit [id=0,name=customer4, credit=45] </span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] TaskExecutorRepeatTemplate:272 - run method end try</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] ResultHolderResultQueue:99 - Put this in queue org.springframework.batch.repeat.support.TaskExecutorRepeatTemplate$ExecutingRunnable@1635497b</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:139 - Take value: isContinuable = true</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:140 - Queue size after take: 0</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:143 - Before count -- : 5</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:145 - After count -- : 4</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:178 - Get org.springframework.batch.repeat.support.TaskExecutorRepeatTemplate$ExecutingRunnable@1635497b from queue.take()</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] ResultHolderResultQueue:105 - Notify all</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:192 - Result status</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-2] TaskExecutorRepeatTemplate:272 - run method end try</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] TaskExecutorRepeatTemplate:285 - run method end finally</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:176 - Running queue.take()</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-2] ResultHolderResultQueue:99 - Put this in queue org.springframework.batch.repeat.support.TaskExecutorRepeatTemplate$ExecutingRunnable@62c14c4b</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] TaskExecutorRepeatTemplate:259 - run method start</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:137 - Queue size before take: 0</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:139 - Take value: isContinuable = true</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:140 - Queue size after take: 0</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:143 - Before count -- : 4</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:145 - After count -- : 3</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] RepeatTemplate:133 - Entering RepeatStatus.iterate</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] RepeatTemplate:161 - RepeatStatus.executeInternal</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:178 - Get org.springframework.batch.repeat.support.TaskExecutorRepeatTemplate$ExecutingRunnable@62c14c4b from queue.take()</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-2] ResultHolderResultQueue:105 - Notify all</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:192 - Result status</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-2] TaskExecutorRepeatTemplate:285 - run method end finally</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] FlatFileItemReader:188 - Reading customer7,10</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:176 - Running queue.take()</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:137 - Queue size before take: 0</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-1] TaskExecutorRepeatTemplate:272 - run method end try</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] FlatFileItemReader:188 - Reading customer8,20</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-1] ResultHolderResultQueue:99 - Put this in queue org.springframework.batch.repeat.support.TaskExecutorRepeatTemplate$ExecutingRunnable@77723a53</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:139 - Take value: isContinuable = true</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:140 - Queue size after take: 0</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] RepeatTemplate:224 - result is CONTINUABLE</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:143 - Before count -- : 3</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] RepeatTemplate:383 - Entering no-op waitForResults</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:145 - After count -- : 2</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] RepeatTemplate:229 - Get result CONTINUABLE</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-1] ResultHolderResultQueue:105 - Notify all</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:178 - Get org.springframework.batch.repeat.support.TaskExecutorRepeatTemplate$ExecutingRunnable@77723a53 from queue.take()</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] RepeatTemplate:149 - Quit RepeatStatus.iterate</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-1] TaskExecutorRepeatTemplate:285 - run method end finally</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:192 - Result status</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:176 - Running queue.take()</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] AbstractFileItemWriter:76 - Writing CustomerCredit [id=0,name=customer7, credit=15] </span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:137 - Queue size before take: 0</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] AbstractFileItemWriter:76 - Writing CustomerCredit [id=0,name=customer8, credit=25] </span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] TaskExecutorRepeatTemplate:272 - run method end try</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] ResultHolderResultQueue:99 - Put this in queue org.springframework.batch.repeat.support.TaskExecutorRepeatTemplate$ExecutingRunnable@70eedc13</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:139 - Take value: isContinuable = true</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:140 - Queue size after take: 0</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:143 - Before count -- : 2</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:145 - After count -- : 1</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:178 - Get org.springframework.batch.repeat.support.TaskExecutorRepeatTemplate$ExecutingRunnable@70eedc13 from queue.take()</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] ResultHolderResultQueue:105 - Notify all</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:192 - Result status</span><br><span class="line">2025-07-06 19:03:56 INFO  [threadPoolTaskExecutor-3] TaskExecutorRepeatTemplate:285 - run method end finally</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] TaskExecutorRepeatTemplate:176 - Running queue.take()</span><br><span class="line">2025-07-06 19:03:56 INFO  [main] ResultHolderResultQueue:137 - Queue size before take: 0</span><br></pre></td></tr></table></figure>
</details>


<h4 id="1-4-Running-explanation"><a href="#1-4-Running-explanation" class="headerlink" title="1.4 Running explanation"></a>1.4 Running explanation</h4><ol>
<li>Main thread goes to TaskExecutorRepeatTemplate</li>
<li>Main thread submit 4 times task, see <code>TaskExecutorRepeatTemplate</code></li>
<li>When reached the 5th task, <strong>it goes error due to ThreadPool raised TaskRejectedException, but Spring Batch did nothing</strong>. <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2025-07-06 17:59:55 INFO  [main] TaskExecutorRepeatTemplate:127 - Submitting jobs org.springframework.batch.core.step.tasklet.TaskletStep$2@2ee39e73</span><br><span class="line">2025-07-06 17:59:55 INFO  [threadPoolTaskExecutor-3] TaskExecutorRepeatTemplate:259 - run method start</span><br><span class="line">2025-07-06 17:59:55 INFO  [main] TaskExecutorRepeatTemplate:298 - Handling exception: org.springframework.core.task.TaskRejectedException, caused by: org.springframework.core.task.TaskRejectedException: ExecutorService in active state did not accept task: org.springframework.batch.repeat.support.TaskExecutorRepeatTemplate$ExecutingRunnable@c4d2c44</span><br></pre></td></tr></table></figure></li>
<li>Main thread goes in running queue.take() and stay in waiting</li>
<li>Main thread was waked up due to other three finished thread runs notifyAll()</li>
<li>Main thread expecting 5 result to arrive, currently only 4. It went into waiting or taking then no thread wakes it up.</li>
</ol>
<h3 id="2-Source-code-hint-of-untrustworthy-ThottleLimit"><a href="#2-Source-code-hint-of-untrustworthy-ThottleLimit" class="headerlink" title="2. Source code hint of untrustworthy ThottleLimit"></a>2. <a target="_blank" rel="noopener" href="https://github.com/spring-projects/spring-batch/blob/v5.1.0/spring-batch-infrastructure/src/main/java/org/springframework/batch/repeat/support/TaskExecutorRepeatTemplate.java#L63-L79">Source code hint of untrustworthy ThottleLimit</a></h3><p>It sais: when used with a thread pooled TaskExecutor the thread pool might prevent the throttle limit actually being reached (<strong>so make the core pool size larger than the throttle limit if possible</strong>).</p>
<p>Actually we can also adjust the rejection policy to CallerRunPolicy.</p>
<h3 id="3-Change-to-CallerRunPolicy"><a href="#3-Change-to-CallerRunPolicy" class="headerlink" title="3. Change to CallerRunPolicy"></a>3. Change to CallerRunPolicy</h3><h4 id="3-1-Source-code-branch"><a href="#3-1-Source-code-branch" class="headerlink" title="3.1 Source code branch"></a>3.1 Source code branch</h4><p><a target="_blank" rel="noopener" href="https://github.com/Solodye/spring-batch/tree/test/v5.1.0/deadlock-when-spring-batch-submit-more-task-than-core-pool-size">test/v5.1.0/deadlock-when-spring-batch-submit-more-task-than-core-pool-size</a>.</p>
<h4 id="3-2-Source-code-change"><a href="#3-2-Source-code-change" class="headerlink" title="3.2 Source code change"></a>3.2 Source code change</h4><p>One line change </p>
<h2 id="Important-conceptions-TBD"><a href="#Important-conceptions-TBD" class="headerlink" title="Important conceptions (TBD)"></a>Important conceptions (TBD)</h2><h3 id="RepeatOperator"><a href="#RepeatOperator" class="headerlink" title="RepeatOperator"></a>RepeatOperator</h3><h3 id="Internal-queue-for-storaging-the-result"><a href="#Internal-queue-for-storaging-the-result" class="headerlink" title="Internal queue for storaging the result"></a>Internal queue for storaging the result</h3><h3 id="How-a-Reader-Processor-Writer-to-be-wrapped-into-the-RepearOperator"><a href="#How-a-Reader-Processor-Writer-to-be-wrapped-into-the-RepearOperator" class="headerlink" title="How a Reader - Processor - Writer to be wrapped into the RepearOperator"></a>How a Reader - Processor - Writer to be wrapped into the RepearOperator</h3><h2 id="Overall-task-submittion-TBD"><a href="#Overall-task-submittion-TBD" class="headerlink" title="Overall task submittion (TBD)"></a>Overall task submittion (TBD)</h2>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/06/Spark-2-3-Spark-Left-Outer-SortMergeJoin-time-complexity-analyzation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YzRuntime's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | YzRuntime's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/06/Spark-2-3-Spark-Left-Outer-SortMergeJoin-time-complexity-analyzation/" class="post-title-link" itemprop="url"><Spark 2.3> Spark Left Outer SortMergeJoin time complexity analyzation</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-07-06 13:57:58" itemprop="dateCreated datePublished" datetime="2025-07-06T13:57:58+08:00">2025-07-06</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h1><p>We have <code>left table</code> and <code>right table</code>, after some repartition, there are 1 partition got joined across lots of partitions.</p>
<p>Here are the partition data inside:</p>
<table>
<thead>
<tr>
<th>left table partition L</th>
<th>right table partition R</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>8</td>
<td>7</td>
</tr>
<tr>
<td></td>
<td>10</td>
</tr>
</tbody></table>
<p>Suppose after <code>RDD.zipPartition</code>, L and R should be joined together.</p>
<p>Without <code>Spark codegen</code> opened, how many times Spark will visit the data inside L and R?</p>
<ul>
<li>Will it be <code>L * R</code>? Like <code>3 * 4 = 12</code>?</li>
<li>Will it be <code>L + R</code>? Like <code>3 + 4 = 7</code>?</li>
</ul>
<h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><h2 id="Time-Complexity"><a href="#Time-Complexity" class="headerlink" title="Time Complexity"></a>Time Complexity</h2><p>It is <code>L + R + 2 * C</code></p>
<ul>
<li><code>C is right side joined count</code></li>
</ul>
<h2 id="Spark-Optimization"><a href="#Spark-Optimization" class="headerlink" title="Spark Optimization"></a>Spark Optimization</h2><p>If right side table have lots of column cannot be joined, Spark still need to waste time to iterate it
We can try</p>
<ul>
<li>Add more partition to reduce the chances of the hash collapse</li>
<li>Use appropriate filter before join, to eliminate the chances that Spark iterate records that cannot be joined</li>
<li>Use Bloom Filter when we are not sure how to write the filter</li>
</ul>
<h1 id="Analyze"><a href="#Analyze" class="headerlink" title="Analyze"></a>Analyze</h1><h2 id="1-Download-Spark-source-code"><a href="#1-Download-Spark-source-code" class="headerlink" title="1. Download Spark source code"></a>1. Download Spark source code</h2><ul>
<li>Make sure you have linux environment, my environment is <code>Ubuntu 22.04 LTS</code></li>
<li>Make sure running this command first <code>mvn clean -DskipTests -T 20 package</code>, eliminate all the maven compile issue, then refresh maven import</li>
<li>Spark version: 2.3, which means you need to change to <code>branch-2.3</code></li>
</ul>
<h2 id="2-Change-a-unit-test"><a href="#2-Change-a-unit-test" class="headerlink" title="2. Change a unit test"></a>2. Change a unit test</h2><h3 id="2-1-Details"><a href="#2-1-Details" class="headerlink" title="2.1 Details"></a>2.1 Details</h3><p>For testing the <code>DataFrame</code> processing, we need to hardcode the data inside a UT.</p>
<p>We pick up the following UT to make this change:</p>
<ul>
<li>Path: <code>sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala</code></li>
<li>Change:<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">  test(<span class="string">&quot;SPARK-20897: cached self-join should not fail&quot;</span>) &#123;</span><br><span class="line">  withSQLConf((<span class="type">SQLConf</span>.<span class="type">AUTO_BROADCASTJOIN_THRESHOLD</span>.key -&gt; <span class="string">&quot;0&quot;</span>), <span class="comment">// force to plan sort merge join</span></span><br><span class="line">    (<span class="type">SQLConf</span>.<span class="type">WHOLESTAGE_CODEGEN_ENABLED</span>.key -&gt; <span class="string">&quot;false&quot;</span> )  <span class="comment">// force Spark not to use CodeGen</span></span><br><span class="line">  ) &#123;</span><br><span class="line">    <span class="comment">// 0x added behind because later we output using print(), it will shows HEX value</span></span><br><span class="line">    <span class="keyword">val</span> df1 = <span class="type">Seq</span>(<span class="number">0x1</span> -&gt; <span class="number">0x1</span>, <span class="number">0x4</span> -&gt; <span class="number">0x4</span>, <span class="number">0x8</span> -&gt; <span class="number">0x8</span>).toDF(<span class="string">&quot;i&quot;</span>, <span class="string">&quot;j&quot;</span>).as(<span class="string">&quot;t1&quot;</span>).coalesce(<span class="number">1</span>) <span class="comment">// coalesce force all the data into one partition</span></span><br><span class="line">    <span class="keyword">val</span> df2 = <span class="type">Seq</span>(<span class="number">0x10</span> -&gt; <span class="number">0x1010</span>, <span class="number">0x4</span> -&gt; <span class="number">0x44</span>, <span class="number">0x4</span> -&gt; <span class="number">0x45</span>, <span class="number">0x7</span> -&gt; <span class="number">0x77</span>)</span><br><span class="line">      .toDF(<span class="string">&quot;i&quot;</span>, <span class="string">&quot;j&quot;</span>).as(<span class="string">&quot;t2&quot;</span>).coalesce(<span class="number">1</span>)</span><br><span class="line">    df1.join(df2, $<span class="string">&quot;t1.i&quot;</span> === $<span class="string">&quot;t2.i&quot;</span>, <span class="string">&quot;left_outer&quot;</span>).explain(<span class="literal">false</span>)</span><br><span class="line">    df1.join(df2, $<span class="string">&quot;t1.i&quot;</span> === $<span class="string">&quot;t2.i&quot;</span>, <span class="string">&quot;left_outer&quot;</span>).show(<span class="number">100</span>, <span class="literal">false</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>Then, we add some log output to Spark source code</p>
<ul>
<li>Path: <code>sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala</code></li>
<li>Change: <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">protected</span> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doExecute</span></span>(): <span class="type">RDD</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">  </span><br><span class="line"><span class="comment">/*... Ignore some unchanged code ...*/</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">case</span> <span class="type">LeftOuter</span> =&gt;</span><br><span class="line">          <span class="keyword">val</span> smjScanner = <span class="keyword">new</span> <span class="type">SortMergeJoinScanner</span>(</span><br><span class="line">            streamedKeyGenerator = createLeftKeyGenerator(),</span><br><span class="line">            bufferedKeyGenerator = createRightKeyGenerator(),</span><br><span class="line">            keyOrdering,</span><br><span class="line">            streamedIter = <span class="type">RowIterator</span>.fromScala(leftIter),</span><br><span class="line">            bufferedIter = <span class="type">RowIterator</span>.fromScala(rightIter),</span><br><span class="line">            inMemoryThreshold,</span><br><span class="line">            spillThreshold</span><br><span class="line">          )</span><br><span class="line">          <span class="keyword">val</span> rightNullRow = <span class="keyword">new</span> <span class="type">GenericInternalRow</span>(right.output.length)</span><br><span class="line">          print(<span class="string">&quot;LeftOuter init in SparkPlan\n&quot;</span>) <span class="comment">// Indicate the Spark init the LeftOuterIterator </span></span><br><span class="line">          <span class="keyword">new</span> <span class="type">LeftOuterIterator</span>(</span><br><span class="line">            smjScanner, rightNullRow, boundCondition, resultProj, numOutputRows).toScala</span><br><span class="line"></span><br><span class="line"><span class="comment">/*... Ignore some unchanged code ...*/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[joins] <span class="class"><span class="keyword">class</span> <span class="title">SortMergeJoinScanner</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">                                           streamedKeyGenerator: <span class="type">Projection</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">                                           bufferedKeyGenerator: <span class="type">Projection</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">                                           keyOrdering: <span class="type">Ordering</span>[<span class="type">InternalRow</span>],</span></span></span><br><span class="line"><span class="params"><span class="class">                                           streamedIter: <span class="type">RowIterator</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">                                           bufferedIter: <span class="type">RowIterator</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">                                           inMemoryThreshold: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">                                           spillThreshold: <span class="type">Int</span></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> streamedRow: <span class="type">InternalRow</span> = _</span><br><span class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> streamedRowKey: <span class="type">InternalRow</span> = _</span><br><span class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> bufferedRow: <span class="type">InternalRow</span> = _</span><br><span class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> bufferedRowKey: <span class="type">InternalRow</span> = _</span><br><span class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> matchJoinKey: <span class="type">InternalRow</span> = _</span><br><span class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> bufferedMatches =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalAppendOnlyUnsafeRowArray</span>(inMemoryThreshold, spillThreshold)</span><br><span class="line"></span><br><span class="line">  print(<span class="string">s&quot;<span class="subst">$&#123;this.hashCode()&#125;</span> SortMergeJoinScanner init\n&quot;</span>) <span class="comment">// Because we need to observe the first time Spark reads the right table, rather than left table, we need to add this log to here</span></span><br><span class="line">  advancedBufferedToRowWithNullFreeJoinKey()</span><br><span class="line"></span><br><span class="line"><span class="comment">/*... Ignore some unchanged code ...*/</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">findNextOuterJoinRows</span></span>(): <span class="type">Boolean</span> = &#123; <span class="comment">// This is the main driver function to drive executor to iterate across all the left and right table</span></span><br><span class="line">    print(<span class="string">&quot;findNextOuterJoinRows() \n&quot;</span>) <span class="comment">// we add a log to identify when it enters this function</span></span><br><span class="line">    <span class="keyword">if</span> (!advancedStreamed()) &#123;</span><br><span class="line">      matchJoinKey = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*... Ignore some unchanged code ...*/</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">advancedStreamed</span></span>(): <span class="type">Boolean</span> = &#123; <span class="comment">// This advanced Streamed drives the left table&#x27;s Iterator to reads the left table</span></span><br><span class="line">    <span class="keyword">if</span> (streamedIter.advanceNext()) &#123;</span><br><span class="line">      streamedRow = streamedIter.getRow</span><br><span class="line">      streamedRowKey = streamedKeyGenerator(streamedRow)</span><br><span class="line">      <span class="comment">// Add logs to identify the left table reading</span></span><br><span class="line">      print(<span class="string">s&quot;<span class="subst">$&#123;this.hashCode()&#125;</span> advancedStreamed() streamedIter.advanceNext()=true streamedRow: <span class="subst">$&#123;streamedRow&#125;</span>\n&quot;</span>)</span><br><span class="line">      <span class="literal">true</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      streamedRow = <span class="literal">null</span></span><br><span class="line">      streamedRowKey = <span class="literal">null</span></span><br><span class="line">      <span class="comment">// Add logs to identify the left table reading</span></span><br><span class="line">      print(<span class="string">s&quot;<span class="subst">$&#123;this.hashCode()&#125;</span> advancedStreamed() streamedIter.advanceNext()=false streamedRow: <span class="subst">$&#123;streamedRow&#125;</span>\n&quot;</span>)</span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">      </span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">advancedBufferedToRowWithNullFreeJoinKey</span></span>(): <span class="type">Boolean</span> = &#123; <span class="comment">// This function drives the right table&#x27;s Iterator to read the right table, and it can skip the empty row key</span></span><br><span class="line">    <span class="keyword">var</span> foundRow: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line">    <span class="keyword">while</span> (!foundRow &amp;&amp; bufferedIter.advanceNext()) &#123;</span><br><span class="line">      bufferedRow = bufferedIter.getRow</span><br><span class="line">      <span class="comment">// Add logs to identify the right table reading</span></span><br><span class="line">      print(<span class="string">s&quot;<span class="subst">$&#123;this.hashCode()&#125;</span> [advancedBufferedToRowWithNullFreeJoinKey() bufferedRow in while: <span class="subst">$&#123;bufferedRow&#125;</span>, &quot;</span>)</span><br><span class="line">      bufferedRowKey = bufferedKeyGenerator(bufferedRow)</span><br><span class="line">      foundRow = !bufferedRowKey.anyNull</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (!foundRow) &#123;</span><br><span class="line">      <span class="comment">// Add logs to identify the right table reading</span></span><br><span class="line">      print(<span class="string">s&quot;<span class="subst">$&#123;this.hashCode()&#125;</span> advancedBufferedToRowWithNullFreeJoinKey() not found Row, current is <span class="subst">$&#123;bufferedRow&#125;</span> ]\n&quot;</span>)</span><br><span class="line">      bufferedRow = <span class="literal">null</span></span><br><span class="line">      bufferedRowKey = <span class="literal">null</span></span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Add logs to identify the right table reading</span></span><br><span class="line">      print(<span class="string">s&quot;<span class="subst">$&#123;this.hashCode()&#125;</span> advancedBufferedToRowWithNullFreeJoinKey() found Row, current is <span class="subst">$&#123;bufferedRow&#125;</span> ]\n&quot;</span>)</span><br><span class="line">      <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferMatchingRows</span></span>(): <span class="type">Unit</span> = &#123; <span class="comment">// This function do the actually &quot;link&quot; between left joined data and right joined data </span></span><br><span class="line">    assert(streamedRowKey != <span class="literal">null</span>)</span><br><span class="line">    assert(!streamedRowKey.anyNull)</span><br><span class="line">    assert(bufferedRowKey != <span class="literal">null</span>)</span><br><span class="line">    assert(!bufferedRowKey.anyNull)</span><br><span class="line">    assert(keyOrdering.compare(streamedRowKey, bufferedRowKey) == <span class="number">0</span>)</span><br><span class="line">    matchJoinKey = streamedRowKey.copy()</span><br><span class="line">    bufferedMatches.clear()</span><br><span class="line">    do &#123;</span><br><span class="line">      <span class="comment">// When left record and right record get joined, right record will be stored inside the bufferedRow</span></span><br><span class="line">      <span class="comment">// It will be re-iterated later</span></span><br><span class="line">      print(<span class="string">s&quot;<span class="subst">$&#123;this.hashCode()&#125;</span> bufferedMatches.add(bufferedRow.asInstanceOf[UnsafeRow])\n&quot;</span>)</span><br><span class="line">      bufferedMatches.add(bufferedRow.asInstanceOf[<span class="type">UnsafeRow</span>])</span><br><span class="line">      advancedBufferedToRowWithNullFreeJoinKey()</span><br><span class="line">    &#125; <span class="keyword">while</span> (bufferedRow != <span class="literal">null</span> &amp;&amp; keyOrdering.compare(streamedRowKey, bufferedRowKey) == <span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">LeftOuterIterator</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">                                   smjScanner: <span class="type">SortMergeJoinScanner</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">                                   rightNullRow: <span class="type">InternalRow</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">                                   boundCondition: <span class="type">InternalRow</span> =&gt; <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">                                   resultProj: <span class="type">InternalRow</span> =&gt; <span class="type">InternalRow</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">                                   numOutputRows: <span class="type">SQLMetric</span></span>)</span></span><br><span class="line">    <span class="keyword">extends</span> <span class="type">OneSideOuterIterator</span>(</span><br><span class="line">      smjScanner, rightNullRow, boundCondition, resultProj, numOutputRows) &#123;</span><br><span class="line">    <span class="comment">// We add a log here to verify that, 1. the LeftOuterJoin is activated</span></span><br><span class="line">    <span class="comment">// 2. Record the hashcode</span></span><br><span class="line">    print(<span class="string">s&quot;<span class="subst">$&#123;this.hashCode()&#125;</span> LeftOuterIterator init\n&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setStreamSideOutput</span></span>(row: <span class="type">InternalRow</span>): <span class="type">Unit</span> = joinedRow.withLeft(row)</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setBufferedSideOutput</span></span>(row: <span class="type">InternalRow</span>): <span class="type">Unit</span> = joinedRow.withRight(row)</span><br><span class="line"></span><br><span class="line"><span class="comment">/*... Ignore some unchanged code ...*/</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">advanceStream</span></span>(): <span class="type">Boolean</span> = &#123;</span><br><span class="line">      print(<span class="string">s&quot;<span class="subst">$&#123;this.hashCode()&#125;</span> OneSideOuterIterator.advanceStream()\n&quot;</span>) <span class="comment">// This advanceStream() again, it is LeftOuterIterator&#x27;s advanceStream</span></span><br><span class="line">      rightMatchesIterator = <span class="literal">null</span></span><br><span class="line">      <span class="keyword">if</span> (smjScanner.findNextOuterJoinRows()) &#123;</span><br><span class="line">        setStreamSideOutput(smjScanner.getStreamedRow)</span><br><span class="line">        <span class="keyword">if</span> (smjScanner.getBufferedMatches.isEmpty) &#123;</span><br><span class="line">          setBufferedSideOutput(bufferedSideNullRow)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// Find the next row in the buffer that satisfied the bound condition</span></span><br><span class="line">          <span class="keyword">if</span> (!advanceBufferUntilBoundConditionSatisfied()) &#123;</span><br><span class="line">            setBufferedSideOutput(bufferedSideNullRow)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="literal">true</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// Stream has been exhausted</span></span><br><span class="line">        <span class="literal">false</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// This function will iterate the bufferMatches again, to make the real joined record and output them</span></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">advanceBufferUntilBoundConditionSatisfied</span></span>(): <span class="type">Boolean</span> = &#123; </span><br><span class="line">      <span class="comment">// Add log here to prove the advanceBufferUntilBoundConditionSatisfied will be called</span></span><br><span class="line">      print(<span class="string">s&quot;<span class="subst">$&#123;this.hashCode()&#125;</span> OneSideOuterIterator.advanceBufferUntilBoundConditionSatisfied()\n&quot;</span>)</span><br><span class="line">      <span class="keyword">var</span> foundMatch: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line">      <span class="keyword">if</span> (rightMatchesIterator == <span class="literal">null</span>) &#123;</span><br><span class="line">        rightMatchesIterator = smjScanner.getBufferedMatches.generateIterator()</span><br><span class="line">        <span class="comment">// Add log here to see when it get the bufferMatches</span></span><br><span class="line">        print(<span class="string">s&quot;<span class="subst">$&#123;this.hashCode()&#125;</span> advanceBufferUntilBoundConditionSatisfied() arightMatchesIterator = smjScanner.getBufferedMatches.generateIterator() = <span class="subst">$&#123;rightMatchesIterator.hashCode()&#125;</span> \n&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">while</span> (!foundMatch &amp;&amp; rightMatchesIterator.hasNext) &#123;</span><br><span class="line">        setBufferedSideOutput(rightMatchesIterator.next())</span><br><span class="line">        <span class="keyword">val</span> nextMatch = rightMatchesIterator.next()</span><br><span class="line">        <span class="comment">// Add log here to see the content in bufferMatches, to prove that, bufferMatches stores the joined right table records</span></span><br><span class="line">        print(<span class="string">s&quot;rightMatchesIterator <span class="subst">$&#123;rightMatchesIterator.hashCode()&#125;</span> has <span class="subst">$&#123;nextMatch&#125;</span>&quot;</span>)</span><br><span class="line">        setBufferedSideOutput(nextMatch)</span><br><span class="line">        foundMatch = boundCondition(joinedRow)</span><br><span class="line">      &#125;</span><br><span class="line">      foundMatch</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// LeftOuterIterator&#x27;s advanceNext always try output the joined record first, then try to drive its advanceStream, then to drive SortMergeJoinScanner.findNextOuterJoinRows()</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">advanceNext</span></span>(): <span class="type">Boolean</span> = &#123;</span><br><span class="line">      print(<span class="string">s&quot;<span class="subst">$&#123;this.hashCode()&#125;</span> OneSideOuterIterator.advanceNext()\n&quot;</span>)</span><br><span class="line">      <span class="keyword">val</span> r = advanceBufferUntilBoundConditionSatisfied() || advanceStream()</span><br><span class="line">      <span class="keyword">if</span> (r) numOutputRows += <span class="number">1</span></span><br><span class="line">      r</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getRow</span></span>: <span class="type">InternalRow</span> = resultProj(joinedRow)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="2-2-GitHub-ref-link"><a href="#2-2-GitHub-ref-link" class="headerlink" title="2.2 GitHub ref link"></a>2.2 GitHub ref link</h3><p>Please refer to this Commit:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/Solodye/spark/pull/1/commits/bec260a393d3ddb0d8a7abf8967d2209d021b96f?diff=unified&amp;w=0">https://github.com/Solodye/spark/pull/1/commits/bec260a393d3ddb0d8a7abf8967d2209d021b96f?diff=unified&amp;w=0</a></li>
</ul>
<h2 id="3-Run-and-analyze-the-log"><a href="#3-Run-and-analyze-the-log" class="headerlink" title="3. Run and analyze the log"></a>3. Run and analyze the log</h2><table>
<thead>
<tr>
<th>left side log trace</th>
<th>left side data visit</th>
<th>left table partition x</th>
<th>right table partition y</th>
<th>right side data visit</th>
<th>right side log trace</th>
</tr>
</thead>
<tbody><tr>
<td><code>/* 2. Left #1 */</code></td>
<td>LeftOuterIterator call its .advanceStream(), then drives findNextOuterJoinRows() call left table’s .advanceNext()</td>
<td>1 -&gt; 1</td>
<td>4 -&gt; 44</td>
<td>1. advancedBufferedToRowWithNullFreeJoinKey() when SortMergeJoinScanner init <br/> 2. This record was iterated <strong>AGAIN</strong> inside the buffer</td>
<td>1. <code>/* 1. Right #1 */</code>  <br/> 2. <code>/* 6. Right #1 */</code></td>
</tr>
<tr>
<td><code>/* 3. Left #2 */</code></td>
<td>After the 1st record cannot be joined, someone want to iterate again, so LeftOuterIterator’s advanceNext() was called, then left table’s advanceStream() also been called</td>
<td>4 -&gt; 4</td>
<td>4 -&gt; 45</td>
<td>1. advancedBufferedToRowWithNullFreeJoinKey() when doing the join <br/>  2. This record was iterated <strong>AGAIN</strong> inside the buffer</td>
<td>1. <code>/* 4. Right #2 */ </code> <br/>   2. <code>/* 7. Right #2 */</code></td>
</tr>
<tr>
<td><code>/* 8. Left #3 */</code></td>
<td>advancedStreamed(), then it hasn’t been joined</td>
<td>8 -&gt; 8</td>
<td>7 -&gt; 77</td>
<td>advancedBufferedToRowWithNullFreeJoinKey() when doing the join, then it hasn’t been joined</td>
<td><code>/* 5. Right #3 */</code></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>10 -&gt; 1010</td>
<td>advancedBufferedToRowWithNullFreeJoinKey(),  then it hasn’t been joined</td>
<td><code>/* 9. Right #4 */ </code></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Object</th>
<th>HashCode</th>
</tr>
</thead>
<tbody><tr>
<td>SortMergeJoinScanner</td>
<td>104408849</td>
</tr>
<tr>
<td>LeftOuterIterator</td>
<td>165384710</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">/home/yzruntime/.jdks/corretto-1.8.0_402/bin/java -javaagent:/opt/idea-IC-233.14475.28/lib/idea_rt.jar=42213:/opt/idea-IC-233.14475.28/bin -Dfile.encoding=UTF-8 -classpath /home/yzruntime/.local/share/JetBrains/IdeaIC2023.3/Scala/lib/runners.jar:/home/yzruntime/.jdks/corretto-1.8.0_402/jre/lib/charsets.jar:/home/yzruntime/.jdks/corretto-1.8.0_402/jre/lib/ext/cldrdata.jar:/home/yzruntime/.jdks/corretto-1.8.0_402/jre/lib/ext/dnsns.jar:/home/yzruntime/.jdks/corretto-1.8.0_402/jre/lib/ext/jaccess.jar:/home/yzruntime/.jdks/corretto-1.8.0_402/jre/lib/ext/jfxrt.jar:/home/yzruntime/.jdks/corretto-1.8.0_402/jre/lib/ext/localedata.jar:/home/yzruntime/.jdks/corretto-1.8.0_402/jre/lib/ext/nashorn.jar:/home/yzruntime/.jdks/corretto-1.8.0_402/jre/lib/ext/sunec.jar:/home/yzruntime/.jdks/corretto-1.8.0_402/jre/lib/ext/sunjce_provider.jar:/home/yzruntime/.jdks/corretto-1.8.0_402/jre/lib/ext/sunpkcs11.jar:/home/yzruntime/.jdks/corretto-1.8.0_402/jre/lib/ext/zipfs.jar:/home/yzruntime/.jdks/corretto-1.8.0_402/jre/lib/jce.jar:/home/yzruntime/.jdks/corretto-1.8.0_402/jre/lib/jfr.jar:/home/yzruntime/.jdks/corretto-1.8.0_402/jre/lib/jfxswt.jar:/home/yzruntime/.jdks/corretto-1.8.0_402/jre/lib/jsse.jar:/home/yzruntime/.jdks/corretto-1.8.0_402/jre/lib/management-agent.jar:/home/yzruntime/.jdks/corretto-1.8.0_402/jre/lib/resources.jar:/home/yzruntime/.jdks/corretto-1.8.0_402/jre/lib/rt.jar:/home/yzruntime/GitProjSrc/_forked_spark/spark/sql/core/target/scala-2.11/test-classes:/home/yzruntime/GitProjSrc/_forked_spark/spark/sql/core/target/scala-2.11/classes:/home/yzruntime/.m2/repository/com/univocity/univocity-parsers/2.5.9/univocity-parsers-2.5.9.jar:/home/yzruntime/GitProjSrc/_forked_spark/spark/common/sketch/target/scala-2.11/classes:/home/yzruntime/GitProjSrc/_forked_spark/spark/core/target/scala-2.11/classes:/home/yzruntime/.m2/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/home/yzruntime/.m2/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7.jar:/home/yzruntime/.m2/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/home/yzruntime/.m2/repository/com/twitter/chill_2.11/0.8.4/chill_2.11-0.8.4.jar:/home/yzruntime/.m2/repository/com/twitter/chill-java/0.8.4/chill-java-0.8.4.jar:/home/yzruntime/.m2/repository/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.jar:/home/yzruntime/.m2/repository/org/apache/hadoop/hadoop-common/2.6.5/hadoop-common-2.6.5.jar:/home/yzruntime/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/yzruntime/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/yzruntime/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/yzruntime/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/yzruntime/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/yzruntime/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/yzruntime/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/yzruntime/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/home/yzruntime/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/home/yzruntime/.m2/repository/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar:/home/yzruntime/.m2/repository/org/apache/hadoop/hadoop-auth/2.6.5/hadoop-auth-2.6.5.jar:/home/yzruntime/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/yzruntime/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/yzruntime/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/yzruntime/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/yzruntime/.m2/repository/org/apache/curator/curator-client/2.6.0/curator-client-2.6.0.jar:/home/yzruntime/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/yzruntime/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.6.5/hadoop-hdfs-2.6.5.jar:/home/yzruntime/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/yzruntime/.m2/repository/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/home/yzruntime/.m2/repository/xml-apis/xml-apis/1.4.01/xml-apis-1.4.01.jar:/home/yzruntime/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.5/hadoop-mapreduce-client-app-2.6.5.jar:/home/yzruntime/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.5/hadoop-mapreduce-client-common-2.6.5.jar:/home/yzruntime/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.6.5/hadoop-yarn-client-2.6.5.jar:/home/yzruntime/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.6.5/hadoop-yarn-server-common-2.6.5.jar:/home/yzruntime/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.5/hadoop-mapreduce-client-shuffle-2.6.5.jar:/home/yzruntime/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.6.5/hadoop-yarn-api-2.6.5.jar:/home/yzruntime/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.5/hadoop-mapreduce-client-core-2.6.5.jar:/home/yzruntime/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.6.5/hadoop-yarn-common-2.6.5.jar:/home/yzruntime/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/home/yzruntime/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/home/yzruntime/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar:/home/yzruntime/.m2/repository/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar:/home/yzruntime/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.5/hadoop-mapreduce-client-jobclient-2.6.5.jar:/home/yzruntime/.m2/repository/org/apache/hadoop/hadoop-annotations/2.6.5/hadoop-annotations-2.6.5.jar:/home/yzruntime/GitProjSrc/_forked_spark/spark/launcher/target/scala-2.11/classes:/home/yzruntime/GitProjSrc/_forked_spark/spark/common/kvstore/target/scala-2.11/classes:/home/yzruntime/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/yzruntime/GitProjSrc/_forked_spark/spark/common/network-common/target/scala-2.11/classes:/home/yzruntime/GitProjSrc/_forked_spark/spark/common/network-shuffle/target/scala-2.11/classes:/home/yzruntime/GitProjSrc/_forked_spark/spark/common/unsafe/target/scala-2.11/classes:/home/yzruntime/.m2/repository/net/java/dev/jets3t/jets3t/0.9.4/jets3t-0.9.4.jar:/home/yzruntime/.m2/repository/org/apache/httpcomponents/httpcore/4.4.8/httpcore-4.4.8.jar:/home/yzruntime/.m2/repository/org/apache/httpcomponents/httpclient/4.5.4/httpclient-4.5.4.jar:/home/yzruntime/.m2/repository/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/yzruntime/.m2/repository/org/bouncycastle/bcprov-jdk15on/1.58/bcprov-jdk15on-1.58.jar:/home/yzruntime/.m2/repository/com/jamesmurty/utils/java-xmlbuilder/1.1/java-xmlbuilder-1.1.jar:/home/yzruntime/.m2/repository/net/iharder/base64/2.3.8/base64-2.3.8.jar:/home/yzruntime/.m2/repository/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.jar:/home/yzruntime/.m2/repository/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/home/yzruntime/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/yzruntime/.m2/repository/org/eclipse/jetty/jetty-plus/9.3.24.v20180605/jetty-plus-9.3.24.v20180605.jar:/home/yzruntime/.m2/repository/org/eclipse/jetty/jetty-webapp/9.3.24.v20180605/jetty-webapp-9.3.24.v20180605.jar:/home/yzruntime/.m2/repository/org/eclipse/jetty/jetty-xml/9.3.24.v20180605/jetty-xml-9.3.24.v20180605.jar:/home/yzruntime/.m2/repository/org/eclipse/jetty/jetty-jndi/9.3.24.v20180605/jetty-jndi-9.3.24.v20180605.jar:/home/yzruntime/.m2/repository/org/eclipse/jetty/jetty-security/9.3.24.v20180605/jetty-security-9.3.24.v20180605.jar:/home/yzruntime/.m2/repository/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar:/home/yzruntime/.m2/repository/org/eclipse/jetty/jetty-server/9.3.24.v20180605/jetty-server-9.3.24.v20180605.jar:/home/yzruntime/.m2/repository/org/eclipse/jetty/jetty-io/9.3.24.v20180605/jetty-io-9.3.24.v20180605.jar:/home/yzruntime/.m2/repository/org/eclipse/jetty/jetty-http/9.3.24.v20180605/jetty-http-9.3.24.v20180605.jar:/home/yzruntime/.m2/repository/org/eclipse/jetty/jetty-continuation/9.3.24.v20180605/jetty-continuation-9.3.24.v20180605.jar:/home/yzruntime/.m2/repository/org/eclipse/jetty/jetty-proxy/9.3.24.v20180605/jetty-proxy-9.3.24.v20180605.jar:/home/yzruntime/.m2/repository/org/eclipse/jetty/jetty-client/9.3.24.v20180605/jetty-client-9.3.24.v20180605.jar:/home/yzruntime/.m2/repository/org/eclipse/jetty/jetty-servlets/9.3.24.v20180605/jetty-servlets-9.3.24.v20180605.jar:/home/yzruntime/.m2/repository/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/home/yzruntime/.m2/repository/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.jar:/home/yzruntime/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/yzruntime/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/yzruntime/.m2/repository/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar:/home/yzruntime/.m2/repository/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/home/yzruntime/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/home/yzruntime/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/yzruntime/.m2/repository/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar:/home/yzruntime/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/yzruntime/.m2/repository/org/xerial/snappy/snappy-java/1.1.2.6/snappy-java-1.1.2.6.jar:/home/yzruntime/.m2/repository/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.jar:/home/yzruntime/.m2/repository/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.jar:/home/yzruntime/.m2/repository/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.jar:/home/yzruntime/.m2/repository/org/roaringbitmap/shims/0.7.45/shims-0.7.45.jar:/home/yzruntime/.m2/repository/commons-net/commons-net/2.2/commons-net-2.2.jar:/home/yzruntime/.m2/repository/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar:/home/yzruntime/.m2/repository/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/home/yzruntime/.m2/repository/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/home/yzruntime/.m2/repository/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/home/yzruntime/.m2/repository/org/scala-lang/scalap/2.11.8/scalap-2.11.8.jar:/home/yzruntime/.m2/repository/org/scala-lang/scala-compiler/2.11.8/scala-compiler-2.11.8.jar:/home/yzruntime/.m2/repository/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/home/yzruntime/.m2/repository/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/home/yzruntime/.m2/repository/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/home/yzruntime/.m2/repository/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/home/yzruntime/.m2/repository/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/home/yzruntime/.m2/repository/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/home/yzruntime/.m2/repository/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/home/yzruntime/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/yzruntime/.m2/repository/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/home/yzruntime/.m2/repository/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/home/yzruntime/.m2/repository/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/home/yzruntime/.m2/repository/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/home/yzruntime/.m2/repository/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/home/yzruntime/.m2/repository/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/home/yzruntime/.m2/repository/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/home/yzruntime/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/home/yzruntime/.m2/repository/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/home/yzruntime/.m2/repository/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.jar:/home/yzruntime/.m2/repository/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.jar:/home/yzruntime/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/yzruntime/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.jar:/home/yzruntime/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.jar:/home/yzruntime/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.jar:/home/yzruntime/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.jar:/home/yzruntime/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.7.1/jackson-module-scala_2.11-2.6.7.1.jar:/home/yzruntime/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.9/jackson-module-paranamer-2.7.9.jar:/home/yzruntime/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/yzruntime/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/yzruntime/.m2/repository/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/home/yzruntime/.m2/repository/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.jar:/home/yzruntime/.m2/repository/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/home/yzruntime/GitProjSrc/_forked_spark/spark/core/target/scala-2.11/test-classes:/home/yzruntime/GitProjSrc/_forked_spark/spark/sql/catalyst/target/scala-2.11/classes:/home/yzruntime/.m2/repository/org/scala-lang/scala-reflect/2.11.8/scala-reflect-2.11.8.jar:/home/yzruntime/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar:/home/yzruntime/.m2/repository/org/codehaus/janino/janino/3.0.8/janino-3.0.8.jar:/home/yzruntime/.m2/repository/org/codehaus/janino/commons-compiler/3.0.8/commons-compiler-3.0.8.jar:/home/yzruntime/.m2/repository/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar:/home/yzruntime/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/home/yzruntime/GitProjSrc/_forked_spark/spark/sql/catalyst/target/scala-2.11/test-classes:/home/yzruntime/GitProjSrc/_forked_spark/spark/common/tags/target/scala-2.11/classes:/home/yzruntime/GitProjSrc/_forked_spark/spark/common/tags/target/scala-2.11/test-classes:/home/yzruntime/.m2/repository/org/apache/orc/orc-core/1.4.4/orc-core-1.4.4-nohive.jar:/home/yzruntime/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/yzruntime/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/yzruntime/.m2/repository/io/airlift/aircompressor/0.8/aircompressor-0.8.jar:/home/yzruntime/.m2/repository/org/apache/orc/orc-mapreduce/1.4.4/orc-mapreduce-1.4.4-nohive.jar:/home/yzruntime/.m2/repository/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/home/yzruntime/.m2/repository/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/yzruntime/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/yzruntime/.m2/repository/org/apache/parquet/parquet-column/1.8.3/parquet-column-1.8.3.jar:/home/yzruntime/.m2/repository/org/apache/parquet/parquet-common/1.8.3/parquet-common-1.8.3.jar:/home/yzruntime/.m2/repository/org/apache/parquet/parquet-encoding/1.8.3/parquet-encoding-1.8.3.jar:/home/yzruntime/.m2/repository/org/apache/parquet/parquet-hadoop/1.8.3/parquet-hadoop-1.8.3.jar:/home/yzruntime/.m2/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar:/home/yzruntime/.m2/repository/org/apache/parquet/parquet-jackson/1.8.3/parquet-jackson-1.8.3.jar:/home/yzruntime/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/yzruntime/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/yzruntime/.m2/repository/org/eclipse/jetty/jetty-servlet/9.3.24.v20180605/jetty-servlet-9.3.24.v20180605.jar:/home/yzruntime/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.jar:/home/yzruntime/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.6.7/jackson-annotations-2.6.7.jar:/home/yzruntime/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.6.7/jackson-core-2.6.7.jar:/home/yzruntime/.m2/repository/org/apache/arrow/arrow-vector/0.8.0/arrow-vector-0.8.0.jar:/home/yzruntime/.m2/repository/org/apache/arrow/arrow-format/0.8.0/arrow-format-0.8.0.jar:/home/yzruntime/.m2/repository/org/apache/arrow/arrow-memory/0.8.0/arrow-memory-0.8.0.jar:/home/yzruntime/.m2/repository/joda-time/joda-time/2.9.3/joda-time-2.9.3.jar:/home/yzruntime/.m2/repository/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.jar:/home/yzruntime/.m2/repository/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.jar:/home/yzruntime/.m2/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/home/yzruntime/.m2/repository/org/scalacheck/scalacheck_2.11/1.13.5/scalacheck_2.11-1.13.5.jar:/home/yzruntime/.m2/repository/org/scala-sbt/test-interface/1.0/test-interface-1.0.jar:/home/yzruntime/.m2/repository/com/h2database/h2/1.4.195/h2-1.4.195.jar:/home/yzruntime/.m2/repository/mysql/mysql-connector-java/5.1.38/mysql-connector-java-5.1.38.jar:/home/yzruntime/.m2/repository/org/postgresql/postgresql/9.4.1207.jre7/postgresql-9.4.1207.jre7.jar:/home/yzruntime/.m2/repository/org/apache/parquet/parquet-avro/1.8.3/parquet-avro-1.8.3.jar:/home/yzruntime/.m2/repository/it/unimi/dsi/fastutil/6.5.7/fastutil-6.5.7.jar:/home/yzruntime/.m2/repository/org/apache/avro/avro/1.8.1/avro-1.8.1.jar:/home/yzruntime/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/yzruntime/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/yzruntime/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/yzruntime/.m2/repository/org/mockito/mockito-core/1.10.19/mockito-core-1.10.19.jar:/home/yzruntime/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/yzruntime/.m2/repository/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/home/yzruntime/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/yzruntime/.m2/repository/org/scalatest/scalatest_2.11/3.0.3/scalatest_2.11-3.0.3.jar:/home/yzruntime/.m2/repository/org/scalactic/scalactic_2.11/3.0.3/scalactic_2.11-3.0.3.jar:/home/yzruntime/.m2/repository/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar:/home/yzruntime/.m2/repository/junit/junit/4.12/junit-4.12.jar:/home/yzruntime/.m2/repository/com/novocode/junit-interface/0.11/junit-interface-0.11.jar org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner -s org.apache.spark.sql.DataFrameSuite -testName &quot;SPARK-20897: cached self-join should not fail&quot; -showProgressMessages true</span><br><span class="line">Testing started at 4:35 pm ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">01:35:55.103 WARN org.apache.spark.util.Utils: Your hostname, yzhost resolves to a loopback address: 127.0.1.1; using 192.168.50.125 instead (on interface wlp5s0)</span><br><span class="line">01:35:55.104 WARN org.apache.spark.util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address</span><br><span class="line">01:35:55.262 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line"></span><br><span class="line">== Physical Plan ==</span><br><span class="line">SortMergeJoin [i#5], [i#16], LeftOuter</span><br><span class="line">:- Sort [i#5 ASC NULLS FIRST], false, 0</span><br><span class="line">:  +- Coalesce 1</span><br><span class="line">:     +- LocalTableScan [i#5, j#6]</span><br><span class="line">+- Sort [i#16 ASC NULLS FIRST], false, 0</span><br><span class="line">   +- Coalesce 1</span><br><span class="line">      +- LocalTableScan [i#16, j#17]</span><br><span class="line">104408849 SortMergeJoinScanner init</span><br><span class="line">/* 1. Right #1 */ 104408849 [advancedBufferedToRowWithNullFreeJoinKey() bufferedRow in while: [0,4,44], 104408849 advancedBufferedToRowWithNullFreeJoinKey() found Row, current is [0,4,44] ] </span><br><span class="line">LeftOuter init in SparkPlan</span><br><span class="line">165384710 LeftOuterIterator init</span><br><span class="line">165384710 OneSideOuterIterator.advanceNext()</span><br><span class="line">165384710 OneSideOuterIterator.advanceBufferUntilBoundConditionSatisfied()</span><br><span class="line">165384710 advanceBufferUntilBoundConditionSatisfied() arightMatchesIterator = smjScanner.getBufferedMatches.generateIterator() = 1252938438 </span><br><span class="line">165384710 OneSideOuterIterator.advanceStream()</span><br><span class="line">findNextOuterJoinRows() </span><br><span class="line">/* 2. Left #1 */ 104408849 advancedStreamed() streamedIter.advanceNext()=true streamedRow: [0,1,1]</span><br><span class="line">165384710 OneSideOuterIterator.advanceNext()</span><br><span class="line">165384710 OneSideOuterIterator.advanceBufferUntilBoundConditionSatisfied()</span><br><span class="line">165384710 advanceBufferUntilBoundConditionSatisfied() arightMatchesIterator = smjScanner.getBufferedMatches.generateIterator() = 2135241262 </span><br><span class="line">165384710 OneSideOuterIterator.advanceStream()</span><br><span class="line">findNextOuterJoinRows() </span><br><span class="line">/* 3. Left #2 */ 104408849 advancedStreamed() streamedIter.advanceNext()=true streamedRow: [0,4,4]</span><br><span class="line">/* 3.1 Right #1 got joined, then it was put inside the buffer */ 104408849 bufferedMatches.add(bufferedRow.asInstanceOf[UnsafeRow])</span><br><span class="line">/* 4. Right #2 */ 104408849 [advancedBufferedToRowWithNullFreeJoinKey() bufferedRow in while: [0,4,45], 104408849 advancedBufferedToRowWithNullFreeJoinKey() found Row, current is [0,4,45] ]</span><br><span class="line">* 4.1 Right #2 got joined, then it was put inside the buffer */ 104408849 bufferedMatches.add(bufferedRow.asInstanceOf[UnsafeRow])</span><br><span class="line">/* 5. Right #3 */ 104408849 [advancedBufferedToRowWithNullFreeJoinKey() bufferedRow in while: [0,7,77], 104408849 advancedBufferedToRowWithNullFreeJoinKey() found Row, current is [0,7,77] ]</span><br><span class="line">165384710 OneSideOuterIterator.advanceBufferUntilBoundConditionSatisfied()</span><br><span class="line">165384710 advanceBufferUntilBoundConditionSatisfied() arightMatchesIterator = smjScanner.getBufferedMatches.generateIterator() = 1621560499 </span><br><span class="line">/* 6. Right #1 */ rightMatchesIterator 1621560499 has [0,4,44]165384710 OneSideOuterIterator.advanceNext()</span><br><span class="line">165384710 OneSideOuterIterator.advanceBufferUntilBoundConditionSatisfied()</span><br><span class="line">/* 7. Right #2 */ rightMatchesIterator 1621560499 has [0,4,45]165384710 OneSideOuterIterator.advanceNext()</span><br><span class="line">165384710 OneSideOuterIterator.advanceBufferUntilBoundConditionSatisfied()</span><br><span class="line">165384710 OneSideOuterIterator.advanceStream()</span><br><span class="line">findNextOuterJoinRows() </span><br><span class="line">/* 8. Left #3 */ 104408849 advancedStreamed() streamedIter.advanceNext()=true streamedRow: [0,8,8]</span><br><span class="line">/* 9. Right #4 */ 104408849 [advancedBufferedToRowWithNullFreeJoinKey() bufferedRow in while: [0,10,1010], 104408849 advancedBufferedToRowWithNullFreeJoinKey() found Row, current is [0,10,1010] ]</span><br><span class="line">165384710 OneSideOuterIterator.advanceNext()</span><br><span class="line">165384710 OneSideOuterIterator.advanceBufferUntilBoundConditionSatisfied()</span><br><span class="line">165384710 advanceBufferUntilBoundConditionSatisfied() arightMatchesIterator = smjScanner.getBufferedMatches.generateIterator() = 2010570488 </span><br><span class="line">165384710 OneSideOuterIterator.advanceStream()</span><br><span class="line">findNextOuterJoinRows() </span><br><span class="line">104408849 advancedStreamed() streamedIter.advanceNext()=false streamedRow: null</span><br><span class="line">+---+---+----+----+</span><br><span class="line">|i  |j  |i   |j   |</span><br><span class="line">+---+---+----+----+</span><br><span class="line">|1  |1  |null|null|</span><br><span class="line">|4  |4  |4   |68  |</span><br><span class="line">|4  |4  |4   |69  |</span><br><span class="line">|8  |8  |null|null|</span><br><span class="line">+---+---+----+----+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a target="_blank" rel="noopener" href="https://www.waitingforcode.com/apache-spark-sql/regression-tests-apache-spark-sql-joins/read">https://www.waitingforcode.com/apache-spark-sql/regression-tests-apache-spark-sql-joins/read</a></li>
<li><a target="_blank" rel="noopener" href="https://use-the-index-luke.com/sql/join/sort-merge-join">https://use-the-index-luke.com/sql/join/sort-merge-join</a></li>
<li><a target="_blank" rel="noopener" href="https://www.waitingforcode.com/apache-spark-sql/sort-merge-join-spark-sql/read#sort-merge_join_in_spark_sql">https://www.waitingforcode.com/apache-spark-sql/sort-merge-join-spark-sql/read#sort-merge_join_in_spark_sql</a></li>
<li>Spark SQL内核剖析– 朱锋，张韶全，黄明</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/06/Learning-Spanish/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YzRuntime's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | YzRuntime's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/06/Learning-Spanish/" class="post-title-link" itemprop="url">Learning Spanish at Spain</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-07-06 13:57:58" itemprop="dateCreated datePublished" datetime="2025-07-06T13:57:58+08:00">2025-07-06</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="2024-04-29-Lunes"><a href="#2024-04-29-Lunes" class="headerlink" title="2024-04-29 Lunes"></a>2024-04-29 Lunes</h1><p><strong>¿De dónde eles?</strong></p>
<p>Norte de China</p>
<p>Alemania</p>
<ul>
<li>alemán</li>
<li>almana</li>
</ul>
<p>Marruecos 摩洛哥</p>
<ul>
<li>marroquí m + f</li>
</ul>
<p>Argetina -f, 也是国家名</p>
<ul>
<li>Argentino -m</li>
</ul>
<p>Francia</p>
<ul>
<li>francés</li>
<li>francesa</li>
</ul>
<p>Soy venezolano/a = soy de Venezuela</p>
<p><strong>¿Cuántos años tienes?</strong></p>
<p>No pareces = don’t looks like
pareces más joven = you looks yonger</p>
<p><strong>¿Cuánto tiempo vas a estudia?</strong> (future tense) = how long you will study</p>
<p>tres semanas</p>
<p>un fin de semana = a weekend</p>
<p>un semestre = un semester, usually means six months</p>
<p><strong>¿En qué trabajas?</strong> = Which job do you do?</p>
<p>Trabajar + como: Trabajo como informático.</p>
<p>Trabajar + en: Trabajo en una empresa.</p>
<p>Estoy jubilado = I am retired</p>
<p>Quiero jubilarme en Ecuador. = I want to retire in Ecuador.</p>
<p>¡Qué suerte! = what luck!, lucky you!</p>
<p>Antes = before</p>
<ul>
<li>Antes era “CEO” de varias empresas internacionales. = I was CEO in several international companies.</li>
</ul>
<p><strong>¿Dónde vives aqi en Madrid?</strong> = Where do you live here in Madrid?</p>
<p>En la Calle Fuencallal = In the Street Fuencallal</p>
<p>calle = street</p>
<p>Vosotros teneís que visitar los barrios de Malasaña Justicia Chueca = forgot what is its meaning</p>
<p>Metro Tribunal</p>
<p>Camino a Sampere = I walk to Sampere.</p>
<p>Muy cerca de Sampere = Very close to Sampere</p>
<p>Vivo en Las Rosas que está mas lejos = I live in the very far away Las Rosas.</p>
<p>lejos = far away</p>
<ul>
<li>cerca de = close to</li>
<li>lejos de = far away from</li>
</ul>
<p>Vivo en el norte de Madrid.</p>
<p>¿Quieres visitado? = Do you want to visit it?</p>
<p>Santiago Bernabéu = 圣地亚哥·伯纳乌足球场是一座位于西班牙马德里的全座椅足球场。</p>
<ul>
<li>sala de trofeos = the room of 奖杯(trophy)</li>
</ul>
<p>edificio = building</p>
<p>antiguo = old</p>
<ul>
<li>El edficio nuevo</li>
<li>El edficio antiguo</li>
</ul>
<p>¿Cómo se dice en español? = how can you speak this in Spanish?</p>
<ul>
<li>Es muy caro = it is very cheap</li>
<li>Es muy barato = it is very expensive</li>
</ul>
<p>Like, love</p>
<ul>
<li>me encanta = I love</li>
<li>me gusta = I like</li>
</ul>
<p>muchas veces = lots of times</p>
<p>simpatico = nice</p>
<p>una vez a la semana = 1 time per week</p>
<p>Vuelvo a Singapur en 3 semanas. = I go back to Singapore after 3 weeks.</p>
<p><strong>¿Qué te gusta hacer en tu tiempo libre?</strong> = What do you like to do in your free time.</p>
<p>Me gusta ver películas en español con subtitulos y series. = I like watching movies in Spanish with subtitles and series.</p>
<p>películas = movies</p>
<p>¿Qué mas? = What else?</p>
<p>jugar al ajedres = play chess</p>
<p>cansado = tired</p>
<p>a mí también = me too</p>
<p>Me gusta hacer senderismo en las montañas. = I like hiking in the mountains.</p>
<p>Me gusta mucho. = I like a lot.</p>
<ul>
<li>el cine = cinema</li>
<li>comer en restarantes</li>
<li>quedar con mis amigos en terrazas (地面, 臺地, 露天平臺，陽台, 梯田;階地, 房子, 排屋，排房) = hang out with my friends outside</li>
<li>enseñar español = teach Spanish</li>
</ul>
<p>¿Cuántos clases de español tienes al dia? = How many classes do you have in a day.</p>
<ul>
<li>clases = class, classroom</li>
</ul>
<p><strong>Sequences</strong></p>
<ul>
<li>1a = la primera = first</li>
<li>2a = la segunda = second</li>
<li>3a = la tercera</li>
<li>4a = la cuarta</li>
<li>5a = la quinta</li>
<li>6a = la sexta </li>
</ul>
<p><strong>OK in Spanish</strong></p>
<ul>
<li>¡Claro!</li>
<li>¡Por supuesto! = of course</li>
</ul>
<p><strong>¿listos? = ready?</strong></p>
<ul>
<li>listo / lista<ul>
<li>meaning 1: a name of metro station</li>
<li>meaning 2: inteligente</li>
<li>meaning 3: ready = preparado/preparada</li>
</ul>
</li>
</ul>
<p><strong>Reflexivo</strong> = 反身動詞</p>
<p><strong>Ser +</strong></p>
<ul>
<li>nacionnalidad<ul>
<li>Soy de Suecia (Soy + de + país o ciudad)</li>
<li>Pedro es sueco</li>
<li><strong>Pedro y Maria son almanes.</strong> (nombre m + nombre f + son + adjectivo plural masculino)</li>
</ul>
</li>
<li>profesión</li>
<li>nombre</li>
</ul>
<p><strong>masculino femenino para la nacionnalidad</strong></p>
<ul>
<li>-o m</li>
<li>-a f</li>
<li>-e m,f</li>
</ul>
<p><strong>trabajar +</strong></p>
<ul>
<li>en una empresa(company)</li>
<li>como informático</li>
</ul>
<p>Jorge es dramaturgo escribe obras de teatro. = Jorge is a playwright, he writes plays</p>
<p>diseñadora = woman designer</p>
<ul>
<li>diseñador - m</li>
<li>diseñadora - f</li>
</ul>
<p>diseñador de moda = fashion designer</p>
<p>Periodista - m, f = 记者</p>
<p>-sta is m + f</p>
<ul>
<li>tenista 网球运动员<ul>
<li>Gabriela Sabatini es tenista</li>
<li>Rafa Nadal es tenista</li>
</ul>
</li>
<li>futbolista</li>
<li>taxista = taxi driver</li>
</ul>
<p>socorrista = lifeguard, protect people when people are swimming</p>
<p>peluaquero/-a = hairdresser</p>
<p>pescadero/-a = fish merchant</p>
<p>fisherman:</p>
<ul>
<li>pescador, pescadores</li>
<li>pescadora, pescadoras</li>
</ul>
<p>cocer = cocinar = to cook</p>
<p>cajero, cajera = cashier</p>
<p>supermercado = supermarket</p>
<p>científico, científica = scientific, scientist (adj or a job)</p>
<p>Mi casero se llama Ng es cocinero y trabaja en una tienda de comida.</p>
<ul>
<li>el casero; la casera = landlord</li>
</ul>
<p>Mi amigos son informaticos en unas empresas de Singapur.</p>
<ul>
<li>我的朋友是新加坡一些公司的计算机科学家。</li>
</ul>
<p>ingeniera - f, ingeniero - m = 工程师
ingeniería = engineering</p>
<p>bombero - m, bombera - f = firefighter</p>
<p>policía, m f the same = police</p>
<ul>
<li>Maria es policía.</li>
<li>Pedro es policía y trabaja en una comisaria de policía.<ul>
<li>comisaría = 警察局</li>
</ul>
</li>
</ul>
<p>repartidor - m, repartidora - f = delivery man/woman</p>
<ul>
<li>Pedor es repartidor de Amazon.</li>
</ul>
<p>cartero - m, cartera - f = postman/postwoman</p>
<ul>
<li>Maria es cartera y trabaja en correos.<ul>
<li>correos = post office</li>
</ul>
</li>
</ul>
<p>conductor - m, conductora - f = conductor, driver</p>
<ul>
<li>Pedro es conductor de autobús.</li>
</ul>
<p>veterinario/-a = veterinarian = 兽医</p>
<p>vegetariano/-a = 素食主义者</p>
<p><strong>something about the class</strong></p>
<ul>
<li>Todos los lunes hay desayuno gratis (Free breakfast is available every Monday)</li>
<li>pausa = rest between 2 classes</li>
</ul>
<p><strong>Tres formas para decir(to say) el nombres:</strong></p>
<ul>
<li>Me llamo Vanesa</li>
<li>Soy Vanesa</li>
<li>Mi nombre es Vanesa</li>
</ul>
<p>estética 美学</p>
<p><strong>mismo</strong></p>
<ul>
<li>你和某个人一样<ul>
<li>Lo mismo que ＋人名</li>
<li>Lo iqual que ＋人名</li>
</ul>
</li>
</ul>
<p>es lo mismo = it’s the same</p>
<p>ahora mismo = right now</p>
<p>enseguida = right away</p>
<p>pasar 度过</p>
<p>dieciséis = diez + seis </p>
<p>esperar 等待</p>
<ul>
<li>espere verde 等路口的红绿灯变绿的标语</li>
</ul>
<p>algo = something</p>
<p>broma = joke</p>
<p>saludar = 打招呼</p>
<p>despedirse = 告别</p>
<ul>
<li>上课讲过这个但记错了</li>
</ul>
<p>pelo = caballo = hair </p>
<ul>
<li>pelo 也可以表示毛发 fur</li>
</ul>
<p>flambear = to blaze = 燃烧</p>
<ul>
<li>fuego = fire</li>
<li>flambear = hacer un fuego grande para cocinar = Make a big fire for cooking</li>
</ul>
<p>hilo = thread</p>
<ul>
<li>pequeña porcíon de tela que se usa coser la ropa = small piece of fabric that is used to sew clothes</li>
</ul>
<p>perchero =  clothes rack, coat stand = 横着的衣架或者大衣架</p>
<p><strong>Ser, Estar</strong></p>
<p>mal visto 难看 不雅观</p>
<ul>
<li>está mal visto </li>
</ul>
<p>característica = feature</p>
<table>
<thead>
<tr>
<th>ser usage</th>
<th>example</th>
</tr>
</thead>
<tbody><tr>
<td>característica: física = 外表</td>
<td>Soy pequeño.</td>
</tr>
<tr>
<td>característica: personalidad = 性格</td>
<td>Soy introvertido. 我内向</td>
</tr>
<tr>
<td>。。</td>
<td>Soy una persona triste/alegre. 我是个悲观/乐观的人</td>
</tr>
<tr>
<td>característica: nacionalidad = 国籍</td>
<td>Soy de China.</td>
</tr>
<tr>
<td>材料 material (¿De qué está hecho? = what is it made of)</td>
<td>El teclado está hecho de plástico. 键盘由塑料制成。</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>estar usage</th>
<th>example</th>
</tr>
</thead>
<tbody><tr>
<td>ubicaciones 地点</td>
<td>El café está cerca de aquí.</td>
</tr>
<tr>
<td>pasajero 持续时间短，昙花一现</td>
<td>Soy introvertido. 我内向</td>
</tr>
<tr>
<td>。。</td>
<td>Estoy triste/alegre 我很伤心/开心</td>
</tr>
</tbody></table>
<ul>
<li><p>tener que</p>
</li>
<li><p>People’s status</p>
<ul>
<li>soltero, soltera = single</li>
<li>casado, casada = married</li>
<li>divorciado, diverciada = divorcee 离婚男/女</li>
<li>tengo novio/novia</li>
<li>tengo pareja = have partner<ul>
<li>pareja: m, f</li>
</ul>
</li>
</ul>
</li>
<li><p>así que = so, therefore</p>
</li>
<li><p>conoces a alguien = meet somebody</p>
</li>
</ul>
<p><strong><strong>to try</strong></strong></p>
<ul>
<li>intentar<ul>
<li>inténtalo = try it</li>
</ul>
</li>
<li>probar<ul>
<li>Proeba esta sopa, está deliciosa.</li>
</ul>
</li>
</ul>
<p><strong>age = edad</strong></p>
<p><strong>to be late</strong></p>
<ul>
<li>llegar tarde<ul>
<li>llega tarde a clase</li>
</ul>
</li>
<li>es muy tarde<ul>
<li>Oh, es muy tarde, me voy.</li>
</ul>
</li>
</ul>
<p><strong>siéntate = have a seat</strong></p>
<p><strong>los diás de la semana</strong></p>
<ul>
<li>el lunes</li>
<li>el martes</li>
<li>el miértes</li>
<li>el jueves</li>
<li>el vienes</li>
</ul>
<p><strong>el finde</strong> = 周末</p>
<ul>
<li>el sábado</li>
<li>el domingo</li>
</ul>
<p><strong>叙述日期时间</strong></p>
<ul>
<li>el 2 de mayo de 1808</li>
</ul>
<p><strong>¿Cuantas vocales tenemos?</strong></p>
<ul>
<li>a e i o u</li>
<li>Hay 5 vocales y 22 consonantes</li>
<li>En españa la b y la v suenan igual</li>
<li>La h es muda. Pero tenemos ch.<ul>
<li>mudo, muda 哑的 (adj)</li>
</ul>
</li>
</ul>
<p><strong>prononce</strong></p>
<ul>
<li><p>suave = 弱</p>
</li>
<li><p>fuerte = strong = 强</p>
</li>
<li><p>g</p>
<ul>
<li>g + a = suave</li>
<li>g + e = j<ul>
<li>gue = suave</li>
</ul>
</li>
<li>g + i = j<ul>
<li>gui = suave</li>
</ul>
</li>
<li>g + o = suave</li>
<li>g + u = suave</li>
<li>g + ü + i : Si la u tiene 2 puntos (two spots) suena. = u 在有两个小点的时候，发音。</li>
</ul>
</li>
<li><p>c</p>
<ul>
<li> suena fuerte</li>
<li>c + a = suena fuerte = ka<ul>
<li>que</li>
<li>qui</li>
<li>co</li>
<li>cu</li>
</ul>
</li>
<li>suena suave<ul>
<li>ce = se = ze</li>
<li>ci = si = zi</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>taller = workshop</strong></p>
<ul>
<li>taller de cocina</li>
<li>~ de literario = 文学工作坊 </li>
<li>~ de pintura = paint shop</li>
<li>~ de mecánico = 汽车修理店</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/06/A-Spark-driver-side-issue-Total-size-of-serialized-results-of-xxxx-tasks-xxxx-x-MB-is-bigger-than-spark-driver-maxResultSize/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YzRuntime's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | YzRuntime's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/06/A-Spark-driver-side-issue-Total-size-of-serialized-results-of-xxxx-tasks-xxxx-x-MB-is-bigger-than-spark-driver-maxResultSize/" class="post-title-link" itemprop="url">A Spark driver side issue: Total size of serialized results of xxxx tasks (xxxx.x MB) is bigger than spark.driver.maxResultSize</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-07-06 13:57:58" itemprop="dateCreated datePublished" datetime="2025-07-06T13:57:58+08:00">2025-07-06</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Issue-detail"><a href="#Issue-detail" class="headerlink" title="Issue detail"></a>Issue detail</h1><p>After the shuffle partition changed from 800 to 3200, a non-joining, non-aggregation job, without any collect() inside Spark driver, collapsed by the following error</p>
<table>
<thead>
<tr>
<th>Config</th>
<th>Detail</th>
</tr>
</thead>
<tbody><tr>
<td>Spark Version</td>
<td>2.3.2</td>
</tr>
<tr>
<td>DataFrame count</td>
<td>77 million</td>
</tr>
<tr>
<td>DataFrame Column numbers</td>
<td>Around 20 columns</td>
</tr>
<tr>
<td>Spark Submit mode</td>
<td>Yarn-Cluster</td>
</tr>
<tr>
<td>Driver Memory</td>
<td>4 GB</td>
</tr>
<tr>
<td>Driver Memory Overhead</td>
<td>2 GB</td>
</tr>
</tbody></table>
<details><summary>Log details</summary>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br></pre></td><td class="code"><pre><span class="line">29-03-2024 09:51:54 INFO - 2024-03-29 09:51:54 JST INFO com.xxxxx.xxxxxx.xxx.xxx.executor.ExportExecutor: inputDfPath:hdfs://xxx/user/xxxx/xxx-xxx/xxx_xxx_xxx/data/dt=xxx</span><br><span class="line">29-03-2024 09:51:54 INFO - 2024-03-29 09:51:54 JST INFO com.xxxxx.xxxxxx.xxx.xxx.reader.ReaderImpl: Reading data from HDFS path: [hdfs://xxx/user/xxxx/xxx-xxx/xxx_xxx_xxx/data/dt=xxx] schema: StructType(StructField(xxxxx))</span><br><span class="line">29-03-2024 09:52:34 INFO - 2024-03-29 09:52:34 JST INFO com.xxxxx.xxxxxx.xxx.xxx.writer.WriterImpl: Writing data to HDFS format: [csv] mode:[overwrite] separator:[	] compression:[gzip] path:[hdfs://xxx/user/xxxx/xxx-xxx/xxx_xxx_xxx/data/output/] header:[false] encoding:[UTF-8]</span><br><span class="line">29-03-2024 09:52:34 INFO - 2024-03-29 09:52:34 JST WARN org.apache.spark.util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting &#x27;spark.debug.maxToStringFields&#x27; in SparkEnv.conf.</span><br><span class="line">29-03-2024 09:52:34 INFO - 2024-03-29 09:52:34 JST INFO com.hadoop.compression.lzo.GPLNativeCodeLoader: Loaded native gpl library</span><br><span class="line">29-03-2024 09:52:34 INFO - 2024-03-29 09:52:34 JST INFO com.hadoop.compression.lzo.LzoCodec: Successfully loaded &amp; initialized native-lzo library [hadoop-lzo rev bb4f4d562ec4888b1c6b0dec1ed7bc4b60229496]</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST ERROR org.apache.spark.scheduler.TaskSetManager: Total size of serialized results of 219 tasks (1029.1 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Aborting job null.</span><br><span class="line">29-03-2024 09:53:10 INFO - org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 219 tasks (1029.1 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at scala.Option.foreach(Option.scala:257)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2039)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2060)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2079)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2104)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.RangePartitioner.&lt;init&gt;(Partitioner.scala:171)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:224)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:371)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:605)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:180)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:656)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)</span><br><span class="line">.....</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at com.xxxxx.xxxxxx.xxx.xxx.Boot$.runJob(Boot.scala:55)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at com.xxxxx.xxxxxx.xxx.xxx.Boot$.main(Boot.scala:30)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at com.xxxxx.xxxxxx.xxx.xxx.Boot.main(Boot.scala)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 230.0 in stage 0.0 (TID 261, xxx.xxx.hadoop.server, executor 13): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 364.0 in stage 0.0 (TID 281, xxx.xxx.hadoop.server, executor 13): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 476.0 in stage 0.0 (TID 209, xxx.xxx.hadoop.server, executor 14): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 238.0 in stage 0.0 (TID 259, xxx.xxx.hadoop.server, executor 71): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 464.0 in stage 0.0 (TID 274, xxx.xxx.hadoop.server, executor 8): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 394.0 in stage 0.0 (TID 233, xxx.xxx.hadoop.server, executor 15): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 392.0 in stage 0.0 (TID 212, xxx.xxx.hadoop.server, executor 8): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 513.0 in stage 0.0 (TID 247, xxx.xxx.hadoop.server, executor 33): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 479.0 in stage 0.0 (TID 254, xxx.xxx.hadoop.server, executor 94): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 365.0 in stage 0.0 (TID 211, xxx.xxx.hadoop.server, executor 16): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 458.0 in stage 0.0 (TID 286, xxx.xxx.hadoop.server, executor 16): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 300.0 in stage 0.0 (TID 283, xxx.xxx.hadoop.server, executor 42): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 308.0 in stage 0.0 (TID 279, xxx.xxx.hadoop.server, executor 75): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 134.0 in stage 0.0 (TID 231, xxx.xxx.hadoop.server, executor 75): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 440.0 in stage 0.0 (TID 278, xxx.xxx.hadoop.server, executor 89): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 374.0 in stage 0.0 (TID 277, xxx.xxx.hadoop.server, executor 85): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 343.0 in stage 0.0 (TID 237, xxx.xxx.hadoop.server, executor 85): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 345.0 in stage 0.0 (TID 265, xxx.xxx.hadoop.server, executor 3): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 441.0 in stage 0.0 (TID 267, xxx.xxx.hadoop.server, executor 100): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 502.0 in stage 0.0 (TID 214, xxx.xxx.hadoop.server, executor 79): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 465.0 in stage 0.0 (TID 227, xxx.xxx.hadoop.server, executor 54): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 426.0 in stage 0.0 (TID 226, xxx.xxx.hadoop.server, executor 52): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 359.0 in stage 0.0 (TID 248, xxx.xxx.hadoop.server, executor 100): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 488.0 in stage 0.0 (TID 123, xxx.xxx.hadoop.server, executor 63): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 354.0 in stage 0.0 (TID 205, xxx.xxx.hadoop.server, executor 43): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 506.0 in stage 0.0 (TID 221, xxx.xxx.hadoop.server, executor 43): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 461.0 in stage 0.0 (TID 292, xxx.xxx.hadoop.server, executor 32): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 423.0 in stage 0.0 (TID 253, xxx.xxx.hadoop.server, executor 32): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 367.0 in stage 0.0 (TID 234, xxx.xxx.hadoop.server, executor 89): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 410.0 in stage 0.0 (TID 140, xxx.xxx.hadoop.server, executor 78): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 511.0 in stage 0.0 (TID 164, xxx.xxx.hadoop.server, executor 62): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 484.0 in stage 0.0 (TID 28, xxx.xxx.hadoop.server, executor 12): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 328.0 in stage 0.0 (TID 230, xxx.xxx.hadoop.server, executor 19): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 348.0 in stage 0.0 (TID 258, xxx.xxx.hadoop.server, executor 36): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 333.0 in stage 0.0 (TID 255, xxx.xxx.hadoop.server, executor 27): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 485.0 in stage 0.0 (TID 256, xxx.xxx.hadoop.server, executor 27): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 331.0 in stage 0.0 (TID 271, xxx.xxx.hadoop.server, executor 19): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 411.0 in stage 0.0 (TID 282, xxx.xxx.hadoop.server, executor 36): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 482.0 in stage 0.0 (TID 290, xxx.xxx.hadoop.server, executor 41): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 471.0 in stage 0.0 (TID 288, xxx.xxx.hadoop.server, executor 41): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 490.0 in stage 0.0 (TID 291, xxx.xxx.hadoop.server, executor 93): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 379.0 in stage 0.0 (TID 285, xxx.xxx.hadoop.server, executor 93): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 350.0 in stage 0.0 (TID 216, xxx.xxx.hadoop.server, executor 77): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 396.0 in stage 0.0 (TID 287, xxx.xxx.hadoop.server, executor 60): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 321.0 in stage 0.0 (TID 210, xxx.xxx.hadoop.server, executor 90): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 337.0 in stage 0.0 (TID 235, xxx.xxx.hadoop.server, executor 60): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 90.0 in stage 0.0 (TID 264, xxx.xxx.hadoop.server, executor 86): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 332.0 in stage 0.0 (TID 243, xxx.xxx.hadoop.server, executor 92): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 446.0 in stage 0.0 (TID 284, xxx.xxx.hadoop.server, executor 92): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 496.0 in stage 0.0 (TID 156, xxx.xxx.hadoop.server, executor 81): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 385.0 in stage 0.0 (TID 241, xxx.xxx.hadoop.server, executor 22): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 514.0 in stage 0.0 (TID 266, xxx.xxx.hadoop.server, executor 25): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 462.0 in stage 0.0 (TID 222, xxx.xxx.hadoop.server, executor 90): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 401.0 in stage 0.0 (TID 263, xxx.xxx.hadoop.server, executor 22): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 450.0 in stage 0.0 (TID 228, xxx.xxx.hadoop.server, executor 25): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 225.0 in stage 0.0 (TID 276, xxx.xxx.hadoop.server, executor 4): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 403.0 in stage 0.0 (TID 239, xxx.xxx.hadoop.server, executor 7): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 430.0 in stage 0.0 (TID 280, xxx.xxx.hadoop.server, executor 4): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 505.0 in stage 0.0 (TID 215, xxx.xxx.hadoop.server, executor 91): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 201.0 in stage 0.0 (TID 269, xxx.xxx.hadoop.server, executor 99): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 138.0 in stage 0.0 (TID 257, xxx.xxx.hadoop.server, executor 99): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 314.0 in stage 0.0 (TID 260, xxx.xxx.hadoop.server, executor 47): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 501.0 in stage 0.0 (TID 10, xxx.xxx.hadoop.server, executor 2): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 397.0 in stage 0.0 (TID 272, xxx.xxx.hadoop.server, executor 86): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 453.0 in stage 0.0 (TID 217, xxx.xxx.hadoop.server, executor 45): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 497.0 in stage 0.0 (TID 270, xxx.xxx.hadoop.server, executor 17): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 330.0 in stage 0.0 (TID 268, xxx.xxx.hadoop.server, executor 47): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST ERROR org.apache.spark.scheduler.TaskSetManager: Total size of serialized results of 220 tasks (1036.7 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 368.0 in stage 0.0 (TID 236, xxx.xxx.hadoop.server, executor 9): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 503.0 in stage 0.0 (TID 262, xxx.xxx.hadoop.server, executor 24): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 378.0 in stage 0.0 (TID 289, xxx.xxx.hadoop.server, executor 9): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 475.0 in stage 0.0 (TID 95, xxx.xxx.hadoop.server, executor 83): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 500.0 in stage 0.0 (TID 293, xxx.xxx.hadoop.server, executor 77): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 481.0 in stage 0.0 (TID 249, xxx.xxx.hadoop.server, executor 64): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST WARN org.apache.spark.scheduler.TaskSetManager: Lost task 499.0 in stage 0.0 (TID 273, xxx.xxx.hadoop.server, executor 83): TaskKilled (Stage cancelled)</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST ERROR com.xxxxx.xxxxxx.xxx.xxx.Boot$: boot error</span><br><span class="line">29-03-2024 09:53:10 INFO - org.apache.spark.SparkException: Job aborted.</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:224)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:656)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at com.xxxxx.xxxxxx.xxx.xxx.writer.WriterImpl.writeDataToHDFS(WriterImpl.scala:27)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at com.xxxxx.xxxxxx.xxx.xxx.executor.ExportExecutor.execute(ExportExecutor.scala:45)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at com.xxxxx.xxxxxx.xxx.xxx.executor.base.CommandExecutorImpl$class.execute(CommandExecutor.scala:36)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at com.xxxxx.xxxxxx.xxx.xxx.Boot$$anon$1.execute(Boot.scala:49)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at com.xxxxx.xxxxxx.xxx.xxx.Boot$.runJob(Boot.scala:55)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at com.xxxxx.xxxxxx.xxx.xxx.Boot$.main(Boot.scala:30)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at com.xxxxx.xxxxxx.xxx.xxx.Boot.main(Boot.scala)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)</span><br><span class="line">29-03-2024 09:53:10 INFO - Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 219 tasks (1029.1 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at scala.Option.foreach(Option.scala:257)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2039)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2060)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2079)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2104)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.RangePartitioner.&lt;init&gt;(Partitioner.scala:171)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:224)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:371)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:605)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:180)</span><br><span class="line">29-03-2024 09:53:10 INFO - 	... 36 more</span><br><span class="line">29-03-2024 09:53:10 INFO - 2024-03-29 09:53:10 JST INFO com.xxxxx.xxxxxx.xxx.xxx.Boot$: going to stop the spark</span><br><span class="line">29-03-2024 09:53:17 INFO - 2024-03-29 09:53:17 JST INFO com.xxxxx.xxxxxx.xxx.xxx.Boot$: going to exit from main existStatus:1</span><br><span class="line">29-03-2024 09:53:18 INFO - Process with id 53171 completed unsuccessfully in 105 seconds.</span><br><span class="line">29-03-2024 09:53:18 ERROR - Job run failed!</span><br></pre></td></tr></table></figure>
</details>

<h1 id="Spark-code"><a href="#Spark-code" class="headerlink" title="Spark code"></a>Spark code</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> inputDf = read(schema=???, path=???)</span><br><span class="line"><span class="keyword">val</span> outputDf = inputDf</span><br><span class="line">  .filter(???)</span><br><span class="line">  .transform(applySchema(???.schema))</span><br><span class="line">  .orderBy(id)</span><br><span class="line"></span><br><span class="line">writer.writeOutput(outputDf, ???)</span><br></pre></td></tr></table></figure>

<h1 id="Root-Cause"><a href="#Root-Cause" class="headerlink" title="Root Cause"></a>Root Cause</h1><p>According to Spark plan, it will raise a ranged partitioning at the ending when there is orderBy before writing out.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">== Physical Plan ==</span><br><span class="line">Execute InsertIntoHadoopFsRelationCommand InsertIntoHadoopFsRelationCommand hdfs://xxx/user/xxxx/xxx-xxx/xxx_xxx_xxx/data/output/, false, CSV, ...</span><br><span class="line">+- *(2) Sort [offer_id#0 ASC NULLS FIRST], true, 0</span><br><span class="line">   +- Exchange rangepartitioning(id#0 ASC NULLS FIRST, 3200)</span><br><span class="line">      +- *(1) Project [id#0,...</span><br><span class="line">         +- *(1) Filter (...)</span><br><span class="line">            +- *(1) FileScan csv [id...</span><br></pre></td></tr></table></figure>

<p>Check the ranged partition source code, there was a collect() inside the sourcecode. The collect() could be the main reason that cause the Spark driver failure.</p>
<h1 id="Trial"><a href="#Trial" class="headerlink" title="Trial"></a>Trial</h1><ol>
<li>Try to force a hash partition, not work, it will generate a hash partitioning before range partitioning<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">== Physical Plan ==</span><br><span class="line">Execute InsertIntoHadoopFsRelationCommand InsertIntoHadoopFsRelationCommand hdfs://xxx/user/xxxx/xxx-xxx/xxx_xxx_xxx/data/output/, false, CSV, ...</span><br><span class="line">+- *(2) Sort [offer_id#0 ASC NULLS FIRST], true, 0</span><br><span class="line">   +- Exchange rangepartitioning(id#0 ASC NULLS FIRST, 3200)</span><br><span class="line">      +- Exchange hashpartitioning(id#0, 3200)</span><br><span class="line">         +- *(1) Project [id#0,...</span><br><span class="line">            +- *(1) Filter (...)</span><br><span class="line">               +- *(1) FileScan csv [id...</span><br></pre></td></tr></table></figure></li>
<li>Try removing the <code>spark.driver.maxResultSize</code> by setting <code>spark.driver.maxResultSize=0</code>, got Spark Driver side OOM issue</li>
<li>Try remove orderBy and force a hash partition, it works <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val inputDf = read(schema=???, path=???)</span><br><span class="line">val outputDf = inputDf</span><br><span class="line">.filter(???)</span><br><span class="line">.transform(applySchema(???.schema))</span><br><span class="line">.repartitionBy(id) // &lt;&lt;&lt; Force it repartition</span><br><span class="line"></span><br><span class="line">writer.writeOutput(outputDf, ???)</span><br></pre></td></tr></table></figure></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/06/16/Collection-size-expansion-logic-refactor-in-Java-18/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YzRuntime's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | YzRuntime's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/06/16/Collection-size-expansion-logic-refactor-in-Java-18/" class="post-title-link" itemprop="url">Collection size expansion logic refactor in Java 18</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-06-16 18:29:49" itemprop="dateCreated datePublished" datetime="2022-06-16T18:29:49+08:00">2022-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-07-06 13:57:58" itemprop="dateModified" datetime="2025-07-06T13:57:58+08:00">2025-07-06</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>In Java 18, the array expansion code has huge change compared to Java 8. 
For Java 18 <code>ArrayList</code> and <code>PriorityQueue</code>, their array expansion calls <code>ArraysSupport.newLength()</code>.</p>
<h1 id="ArrayList"><a href="#ArrayList" class="headerlink" title="ArrayList"></a>ArrayList</h1><p>We only discuss the scenario of <strong>appending 1 element</strong>.</p>
<p>Let’s take a look of what inside the <code>ArrayList</code>.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// This is ArrayList&#x27;s the simplest adding function, which aims to append the element</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">add</span><span class="params">(E e)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1. The default user interface modifies modCount directly.</span></span><br><span class="line">    <span class="comment">//    Java 8 did it in ensureCapacityInternal().</span></span><br><span class="line">    modCount++;</span><br><span class="line">    <span class="comment">// 2. elementData is the internal Object[] array that wrapped and maintained by ArrayList</span></span><br><span class="line">    add(e, elementData, size);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(E e, Object[] elementData, <span class="keyword">int</span> s)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 3. s == internal array&#x27;s length means that the internal array full</span></span><br><span class="line">    <span class="keyword">if</span> (s == elementData.length)</span><br><span class="line">        <span class="comment">// 4. Internal array will be changed to a new array with bigger capacity</span></span><br><span class="line">        elementData = grow();</span><br><span class="line">    elementData[s] = e;</span><br><span class="line">    size = s + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>The main responsibility of grow() is to expand the size of internal array <code>elementData</code>.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// When the outside wants to append 1 element, the grow() helper method will set the minCapacity = 1</span></span><br><span class="line"><span class="keyword">private</span> Object[] grow() &#123;</span><br><span class="line">    <span class="keyword">return</span> grow(size + <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> Object[] grow(<span class="keyword">int</span> minCapacity) &#123;</span><br><span class="line">    <span class="keyword">int</span> oldCapacity = elementData.length;</span><br><span class="line">    <span class="comment">// 4. Test if the internal array is empty array</span></span><br><span class="line">    <span class="keyword">if</span> (oldCapacity &gt; <span class="number">0</span> || elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123;</span><br><span class="line">        <span class="comment">// 4.1 If not empty array, calculate a new capacity using ArraysSupport.newLength()</span></span><br><span class="line">        <span class="keyword">int</span> newCapacity = ArraysSupport.newLength(oldCapacity,</span><br><span class="line">                minCapacity - oldCapacity, <span class="comment">/* minimum growth */</span></span><br><span class="line">                oldCapacity &gt;&gt; <span class="number">1</span>           <span class="comment">/* preferred growth */</span>);</span><br><span class="line">        <span class="keyword">return</span> elementData = Arrays.copyOf(elementData, newCapacity);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 4.2 If internal array is empty, in our case grow(size + 1), it will be given a DEFAULT_CAPACITY = 10</span></span><br><span class="line">        <span class="keyword">return</span> elementData = <span class="keyword">new</span> Object[Math.max(DEFAULT_CAPACITY, minCapacity)];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Compared to Java 8, Java 18 removed <code>ensureCapacityInternal()</code>, <code>ensureExplicitCapacity()</code>
 and let <code>grow()</code> to undertake more responsibility.
Previously <code>ensureCapacityInternal()</code> is responsible for testing <code>elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA</code>
 while <code>ensureExplicitCapacity()</code> is responsible for some task of <code>ArraysSupport.newLength()</code>.</p>
<p>Let’s take a look of what inside <code>ArraysSupport</code>.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A soft maximum array length imposed by array growth computations.</span></span><br><span class="line"><span class="comment">// Some JVMs (such as HotSpot) have an implementation limit that will cause OutOfMemoryError(&quot;Requested array size exceeds VM limit&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> SOFT_MAX_ARRAY_LENGTH = Integer.MAX_VALUE - <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5. ArraysSupport.newLength() is responsible for calculate a new length.</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">newLength</span><span class="params">(<span class="keyword">int</span> oldLength, <span class="keyword">int</span> minGrowth, <span class="keyword">int</span> prefGrowth)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 5.1 From our case, minGrowth = minCapacity - oldCapacity = size + 1 - size = 1</span></span><br><span class="line">    <span class="comment">//                    max(minGrowth, prefGrowth) = max(1, 1/2 * size) = size / 2</span></span><br><span class="line">    <span class="comment">//     So the prefLength = 3/2 * size</span></span><br><span class="line">    <span class="keyword">int</span> prefLength = oldLength + Math.max(minGrowth, prefGrowth); <span class="comment">// might overflow</span></span><br><span class="line">    <span class="comment">// 5.2 If 3/2 * size &lt; SOFT_MAX =&gt; size &lt; 2/3 * SOFT_MAX, return the prefLength</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="number">0</span> &lt; prefLength &amp;&amp; prefLength &lt;= SOFT_MAX_ARRAY_LENGTH) &#123;</span><br><span class="line">        <span class="keyword">return</span> prefLength;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> hugeLength(oldLength, minGrowth);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// In our general appending case, minGrowth = 1</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">hugeLength</span><span class="params">(<span class="keyword">int</span> oldLength, <span class="keyword">int</span> minGrowth)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> minLength = oldLength + minGrowth;</span><br><span class="line">    <span class="keyword">if</span> (minLength &lt; <span class="number">0</span>) &#123; <span class="comment">// overflow</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> OutOfMemoryError(</span><br><span class="line">            <span class="string">&quot;Required array length &quot;</span> + oldLength + <span class="string">&quot; + &quot;</span> + minGrowth + <span class="string">&quot; is too large&quot;</span>);</span><br><span class="line">    <span class="comment">// 5.3 If size + 1 &lt;= SOFT_MAX =&gt; size &lt;= SOFT_MAX - 1, return SOFT_MAX</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (minLength &lt;= SOFT_MAX_ARRAY_LENGTH) &#123;</span><br><span class="line">        <span class="keyword">return</span> SOFT_MAX_ARRAY_LENGTH;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 5.4 If size &gt; SOFT_MAX - 1, return 1</span></span><br><span class="line">        <span class="keyword">return</span> minLength;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Java 18 considered the limitation of special Hotspot JVM by setting a <code>SOFT_MAX</code> to <code>Integer.MAX_VALUE - 8</code> 
rather than directly expand the internal array to <code>Integer.MAX_VALUE</code>.</p>
<p>The whole internal array size growing during <strong>appending element one by one</strong> is clear:</p>
<ol>
<li>When <code>1 &lt; size &lt; 2/3 * SOFT_MAX</code>, the <code>elementData</code> will add 1/2 size</li>
<li>When <code>2/3 * SOFT_MAX &lt; size &lt;= SOFT_MAX - 1</code>, adding <code>1/2 * SOFT_MAX</code> may cause the whole <code>elementData.length</code> &gt; <code>Integer.MAX_VALUE</code>.
So it extends <code>elementData</code> to <code>SOFT_MAX</code> directly.</li>
<li>When <code>SOFT_MAX - 1 &lt; size</code>, just keep growing <code>1</code>.<ul>
<li>In most common case, we don’t store the Integer.MAX_VALUE amount of elements into one ArrayList</li>
<li>This growing 1 operation may aim to delay the application crash.</li>
</ul>
</li>
</ol>
<h1 id="PriorityQueue"><a href="#PriorityQueue" class="headerlink" title="PriorityQueue"></a>PriorityQueue</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">grow</span><span class="params">(<span class="keyword">int</span> minCapacity)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> oldCapacity = queue.length;</span><br><span class="line">    <span class="comment">// Double size if small; else grow by 50%</span></span><br><span class="line">    <span class="keyword">int</span> newCapacity = ArraysSupport.newLength(oldCapacity,</span><br><span class="line">            minCapacity - oldCapacity, <span class="comment">/* minimum growth */</span></span><br><span class="line">            oldCapacity &lt; <span class="number">64</span> ? oldCapacity + <span class="number">2</span> : oldCapacity &gt;&gt; <span class="number">1</span></span><br><span class="line">                                       <span class="comment">/* preferred growth */</span>);</span><br><span class="line">    queue = Arrays.copyOf(queue, newCapacity);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>The only difference of <code>grow()</code> strategy between <code>PriorityQueue</code> and <code>ArrayList</code> is the preferred growth. 
<code>ArrayList</code> prefers expand half of its current capacity while <code>PriorityQueue</code> prefers only grow 2 when less than 64 
and grow half when big than 64.  </p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/06/14/Java-collections-src-learning-note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YzRuntime's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | YzRuntime's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/06/14/Java-collections-src-learning-note/" class="post-title-link" itemprop="url">Java collections source code learning note</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-06-14 18:04:22" itemprop="dateCreated datePublished" datetime="2022-06-14T18:04:22+08:00">2022-06-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-07-06 13:57:58" itemprop="dateModified" datetime="2025-07-06T13:57:58+08:00">2025-07-06</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p><a target="_blank" rel="noopener" href="https://docs.google.com/spreadsheets/d/1RrJplFDo4eUL97iUsypflmbEnVFG2VhLAe-VERYTwzg/edit?usp=sharing">https://docs.google.com/spreadsheets/d/1RrJplFDo4eUL97iUsypflmbEnVFG2VhLAe-VERYTwzg/edit?usp=sharing</a></p>
<iframe height="800" width="1200" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vRCSl4ojOL9MT7e9NuO_ZDyExOBPe0sovmbrgBEIqISwKkfdPBOoYVi2ylNUMFChsYAulF6eBi6znii/pubhtml?widget=true&amp;headers=false"></iframe>


<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><h2 id="1-HashMap"><a href="#1-HashMap" class="headerlink" title="1 HashMap"></a>1 HashMap</h2><table>
<thead>
<tr>
<th>Short description</th>
<th>Link</th>
</tr>
</thead>
<tbody><tr>
<td>Time Complexity of HashMap methods</td>
<td><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/4577998/time-complexity-of-hashmap-methods">https://stackoverflow.com/questions/4577998/time-complexity-of-hashmap-methods</a></td>
</tr>
<tr>
<td>Time Complexity of HashMap get()</td>
<td><a target="_blank" rel="noopener" href="https://skyfly.xyz/2020/02/25/Java/HashMapTimeComplexity/">https://skyfly.xyz/2020/02/25/Java/HashMapTimeComplexity/</a></td>
</tr>
<tr>
<td>Understanding HashMap resize()</td>
<td><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000015812438">https://segmentfault.com/a/1190000015812438</a></td>
</tr>
<tr>
<td>The main document that I learn HashMap</td>
<td><a target="_blank" rel="noopener" href="https://blog.csdn.net/v123411739/article/details/78996181">https://blog.csdn.net/v123411739/article/details/78996181</a></td>
</tr>
<tr>
<td>Another document contains some interview questions</td>
<td><a target="_blank" rel="noopener" href="https://blog.csdn.net/v123411739/article/details/115364158">https://blog.csdn.net/v123411739/article/details/115364158</a></td>
</tr>
<tr>
<td>A document with illustration (need to login CSDN)</td>
<td><a target="_blank" rel="noopener" href="https://blog.csdn.net/yinwenjie/article/details/102531402">https://blog.csdn.net/yinwenjie/article/details/102531402</a></td>
</tr>
</tbody></table>
<p>Questions:</p>
<ul>
<li>Didn’t clearly learn the remove() method.</li>
<li>For the tableSizeFor(), it is changed after java 8. But I don’t know how to locate the version of a code change.</li>
<li>After the sharing<ul>
<li>The actual binary calculation progress of the capability of Map. For example capability is 32, so binary is what?<ul>
<li>For example, 111 is 7. But 7 cannot be the capacity.</li>
</ul>
</li>
<li>What is amortized time complexity?</li>
</ul>
</li>
</ul>
<p>Problems solved:</p>
<ul>
<li>The difference between <code>(n - 1) &amp; hash</code> and <code>(e.hash &amp; oldCap) == 0</code> by reading <code>Understanding HashMap resize()</code>.</li>
<li>The time complexity for put() get() delete() is from O(1) to O(k), the key can be a bin’s Linked List or the height of the red black tree.</li>
</ul>
<h2 id="2-ArrayList"><a href="#2-ArrayList" class="headerlink" title="2 ArrayList"></a>2 ArrayList</h2><table>
<thead>
<tr>
<th>Short description</th>
<th>Link</th>
</tr>
</thead>
<tbody><tr>
<td>A main document that I’ve studied</td>
<td><a target="_blank" rel="noopener" href="https://blog.csdn.net/zxt0601/article/details/77281231">https://blog.csdn.net/zxt0601/article/details/77281231</a></td>
</tr>
<tr>
<td>Another ArrayList docs that I didn’t read too much</td>
<td><a target="_blank" rel="noopener" href="https://blog.51cto.com/u_13604316/2673349">https://blog.51cto.com/u_13604316/2673349</a></td>
</tr>
</tbody></table>
<p>Questions:</p>
<ul>
<li>Will System.arrayCopy got overlap? In linux, there are 2 way to copy the memory, one of them can cause overlap and data loss.</li>
<li>Will ArrayList shrink?</li>
<li>What is System.arrayCopy’s space complexity?</li>
</ul>
<p>Problem solved:</p>
<ul>
<li>How ArrayList insert an value?<ul>
<li>grow() -&gt; System.arrayCopy the second half -&gt; Insert the target value to the middle empty slot</li>
</ul>
</li>
</ul>
<h2 id="3-LinkedList"><a href="#3-LinkedList" class="headerlink" title="3 LinkedList"></a>3 LinkedList</h2><table>
<thead>
<tr>
<th>Short description</th>
<th>Link</th>
</tr>
</thead>
<tbody><tr>
<td>A main document that I’ve studied</td>
<td><a target="_blank" rel="noopener" href="https://blog.csdn.net/zxt0601/article/details/77341098">https://blog.csdn.net/zxt0601/article/details/77341098</a></td>
</tr>
</tbody></table>
<p>Questions:</p>
<ul>
<li>Why LinkedList didn’t implement the isEmpty() method directly?</li>
</ul>
<p>Problem solved:</p>
<ul>
<li>How LinkedList treat the value that is null?<ul>
<li>LinkedList can find the index based on the keyObject.equals().
 For example, <code>LinkList.indexOf(Object o)</code>. When we input indexOf(null), it need to avoid calling null.equals().
 And use == instead.</li>
<li><code>LinkedList.contains()</code> also calls <code>indexOf(Object o)</code></li>
</ul>
</li>
</ul>
<h2 id="4-PriorityQueue"><a href="#4-PriorityQueue" class="headerlink" title="4 PriorityQueue"></a>4 PriorityQueue</h2><table>
<thead>
<tr>
<th>Short description</th>
<th>Link</th>
</tr>
</thead>
<tbody><tr>
<td>PriorityQueue source code analysis doc with a little problem</td>
<td><a target="_blank" rel="noopener" href="https://blog.csdn.net/codejas/article/details/85144502">https://blog.csdn.net/codejas/article/details/85144502</a></td>
</tr>
</tbody></table>
<p>This <code>PriorityQueue source code analysis doc with a little problem</code> has a problem of saying that <code>PriorityQueue</code> is min-heap. 
But actually, the <code>PriorityQueue</code> is max heap.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/08/22/Use-global-Map-to-manage-the-alias-of-Spark-DataFrame/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YzRuntime's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | YzRuntime's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/08/22/Use-global-Map-to-manage-the-alias-of-Spark-DataFrame/" class="post-title-link" itemprop="url">Use global Map to manage the alias of Spark DataFrame</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-08-22 18:38:53" itemprop="dateCreated datePublished" datetime="2021-08-22T18:38:53+08:00">2021-08-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-07-06 13:57:58" itemprop="dateModified" datetime="2025-07-06T13:57:58+08:00">2025-07-06</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Pain-points"><a href="#Pain-points" class="headerlink" title="Pain points"></a>Pain points</h1><p>When writing Spark application, we can set the alias for DataFrame while we cannot get a DataFrame’s alias.
 This is because when we use <code>df.alias()</code>, Spark will create a <code>org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias</code>, 
which is a logical plan node. This means that the <code>alias</code> may not be a field inside the DataFrame, it may be just a 
logical plan to be executed in the runtime.</p>
<p>If in one function we set an alias for a DataFrame, then we want to use the alias in another function, we need to declare 
a global fields to let us access the same alias in different functions. This will cause code smell.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">painPoint</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> theAlias = <span class="string">&quot;an_alias&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">function1</span></span>(df: <span class="type">DataFrame</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    df.alias(theAlias)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">function2</span></span>(df: <span class="type">DataFrame</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    df.join(getOsAndPrice.alias(<span class="string">&quot;osAndPrice&quot;</span>), <span class="type">Seq</span>(<span class="string">&quot;id&quot;</span>), <span class="string">&quot;left_outer&quot;</span>)</span><br><span class="line">      .drop(col(<span class="string">s&quot;<span class="subst">$theAlias</span>.os&quot;</span>)) <span class="comment">// Here we should keep &#x27;theAlias&#x27; field</span></span><br><span class="line">    <span class="comment">// drop(s&quot;$theAlias.os&quot;) doesn&#x27;t works, will see the root cause later</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  function2(function1(getOsAndSize)).show(<span class="number">1000</span>, <span class="literal">false</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Resolve"><a href="#Resolve" class="headerlink" title="Resolve"></a>Resolve</h1><p>If we have a global manager to help us manager the alias, the code may change to this.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">painPointResolve</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">function1</span></span>(df: <span class="type">DataFrame</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    df.transform(setAlias(<span class="string">&quot;an_alias&quot;</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">function2</span></span>(df: <span class="type">DataFrame</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    df.join(getOsAndPrice.alias(<span class="string">&quot;osAndPrice&quot;</span>), <span class="type">Seq</span>(<span class="string">&quot;id&quot;</span>), <span class="string">&quot;left_outer&quot;</span>)</span><br><span class="line">      .drop(col(<span class="string">s&quot;<span class="subst">$&#123;getAlias(df)&#125;</span>.os&quot;</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  function2(function1(getOsAndSize)).show(<span class="number">1000</span>, <span class="literal">false</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>So we can use 2 Map to implement this manager. To prevent the parallel issue, we use ConcurrentHashMap as a place to store
 the mapping relationship, and we use synchronized() to make the get or set operator an atomic operator.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> net.i18ndev.spark.alias</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">DataFrame</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.<span class="type">ConcurrentHashMap</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DfAliasManager</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">private</span> <span class="keyword">val</span> toAlias: <span class="type">ConcurrentHashMap</span>[<span class="type">DataFrame</span>, <span class="type">String</span>] = <span class="keyword">new</span> <span class="type">ConcurrentHashMap</span>[<span class="type">DataFrame</span>, <span class="type">String</span>]()</span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">private</span> <span class="keyword">val</span> toDataFrame: <span class="type">ConcurrentHashMap</span>[<span class="type">String</span>, <span class="type">DataFrame</span>] = <span class="keyword">new</span> <span class="type">ConcurrentHashMap</span>[<span class="type">String</span>, <span class="type">DataFrame</span>]()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setAlias</span></span>(alias: <span class="type">String</span>): <span class="type">DataFrame</span> =&gt; <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    df =&gt;</span><br><span class="line">      <span class="keyword">this</span>.synchronized&#123;</span><br><span class="line">        <span class="keyword">val</span> aliasedDf = df.alias(alias)</span><br><span class="line">        toAlias.put(aliasedDf, alias)</span><br><span class="line">        toDataFrame.put(alias, aliasedDf)</span><br><span class="line">        aliasedDf</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getAlias</span></span>(df: <span class="type">DataFrame</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.synchronized&#123;</span><br><span class="line">      toAlias.get(df)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getDataFrame</span></span>(alias: <span class="type">String</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.synchronized&#123;</span><br><span class="line">      toDataFrame.get(alias)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>The code in this article: <a target="_blank" rel="noopener" href="https://github.com/Solodye/spark-demo/blob/main/src/main/scala/net/i18ndev/spark/alias/DfAliasManagerDemo.scala">https://github.com/Solodye/spark-demo/blob/main/src/main/scala/net/i18ndev/spark/alias/DfAliasManagerDemo.scala</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/08/22/How-to-resolve-the-Reference-xxx-is-ambiguous-problem-after-Spark-join/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YzRuntime's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | YzRuntime's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/08/22/How-to-resolve-the-Reference-xxx-is-ambiguous-problem-after-Spark-join/" class="post-title-link" itemprop="url">How to resolve the "Reference 'xxx' is ambiguous" problem after Spark join</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-08-22 17:09:19" itemprop="dateCreated datePublished" datetime="2021-08-22T17:09:19+08:00">2021-08-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-07-06 13:57:58" itemprop="dateModified" datetime="2025-07-06T13:57:58+08:00">2025-07-06</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>When develop table processing Spark applications, we may encounter this issue.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; org.apache.spark.sql.AnalysisException: Reference &#x27;os&#x27; is ambiguous, could be: default.year_table.os, os, osAndSize.os.;</span><br><span class="line">	at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:259)</span><br><span class="line">	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:101)</span><br><span class="line">	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$42.apply(Analyzer.scala:1001)</span><br><span class="line">	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$42.apply(Analyzer.scala:1003)</span><br><span class="line">	at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:53)</span><br><span class="line">	...</span><br></pre></td></tr></table></figure>
<p>The root cause of this issue is our DataFrame have two columns with same column name.
It may be caused by our inappropriate join. To handle this issue, we should remove the redundant columns <strong>before</strong> we join
to make sure there are no columns with same column name, or use <code>alias</code> to remove it after join.</p>
<p>We can use easier code to reproduce this problem.</p>
<h1 id="1-Reproduce"><a href="#1-Reproduce" class="headerlink" title="1. Reproduce"></a>1. Reproduce</h1><p>Imagine company’s business is to sell the <strong>OS</strong>. We already get the basic information from several data sources.</p>
<p>First, we have the most important table <code>year_table</code>, which is ingested from MySQL to Hive.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">+---+-------+----+-------+-----+</span><br><span class="line">|id |os     |year|os     |price|</span><br><span class="line">+---+-------+----+-------+-----+</span><br><span class="line">|1  |ubuntu |2005|ubuntu |50   |</span><br><span class="line">|2  |xiaomi |2010|xiaomi |100  |</span><br><span class="line">|3  |suse   |2015|suse   |150  |</span><br><span class="line">|4  |samsung|2020|samsung|200  |</span><br><span class="line">+---+-------+----+-------+-----+</span><br></pre></td></tr></table></figure>
<p>From the data we can see the OS <code>ubuntu</code> we have was launched in 2015. And we also have 3 other OS.
When we read this data, we should read it from Hive. So we simulate this <code>year_table</code> like this.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getOsAndYear</span></span>: <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> columns = <span class="type">Seq</span>(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;os&quot;</span>, <span class="string">&quot;year&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> data = <span class="type">Seq</span>(</span><br><span class="line">    (<span class="number">1</span>,<span class="string">&quot;ubuntu&quot;</span>, <span class="number">2005</span>),</span><br><span class="line">    (<span class="number">2</span>, <span class="string">&quot;xiaomi&quot;</span>, <span class="number">2010</span>),</span><br><span class="line">    (<span class="number">3</span>, <span class="string">&quot;suse&quot;</span>, <span class="number">2015</span>),</span><br><span class="line">    (<span class="number">4</span>, <span class="string">&quot;samsung&quot;</span>, <span class="number">2020</span>)</span><br><span class="line">  )</span><br><span class="line">  <span class="keyword">val</span> osAndYear = spark.createDataFrame(data).toDF(columns: _*)</span><br><span class="line">  <span class="keyword">val</span> schema = <span class="string">&quot;default&quot;</span></span><br><span class="line">  osAndYear.write.mode(<span class="string">&quot;overwrite&quot;</span>).saveAsTable(<span class="string">s&quot;<span class="subst">$schema</span>.year_table&quot;</span>)</span><br><span class="line">  spark.read.table(<span class="string">s&quot;<span class="subst">$schema</span>.year_table&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Then we have <code>price_table</code> and <code>size_table</code>, which records the price and the space that the OS occupied.
These 2 tables are come from some human static data, which is not stable.
Some name of the OS is null, but ID still can be used.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getOsAndPrice</span></span>: <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> columns = <span class="type">Seq</span>(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;os&quot;</span>, <span class="string">&quot;price&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> data = <span class="type">Seq</span>(</span><br><span class="line">    (<span class="number">1</span>,<span class="string">&quot;ubuntu&quot;</span>, <span class="number">50</span>),</span><br><span class="line">    (<span class="number">2</span>, <span class="string">&quot;xiaomi&quot;</span>, <span class="number">100</span>),</span><br><span class="line">    (<span class="number">3</span>, <span class="literal">null</span>, <span class="number">150</span>),</span><br><span class="line">    (<span class="number">4</span>, <span class="string">&quot;samsung&quot;</span>, <span class="number">200</span>)</span><br><span class="line">  )</span><br><span class="line">  spark.createDataFrame(data).toDF(columns: _*)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getOsAndSize</span></span>: <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> columns = <span class="type">Seq</span>(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;os&quot;</span>, <span class="string">&quot;size&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> data = <span class="type">Seq</span>(</span><br><span class="line">    (<span class="number">1</span>,<span class="string">&quot;ubuntu&quot;</span>, <span class="number">3444</span>),</span><br><span class="line">    (<span class="number">2</span>, <span class="literal">null</span>, <span class="number">4546</span>),</span><br><span class="line">    (<span class="number">3</span>, <span class="string">&quot;suse&quot;</span>, <span class="number">5747</span>),</span><br><span class="line">    (<span class="number">4</span>, <span class="string">&quot;samsung&quot;</span>, <span class="number">687687</span>)</span><br><span class="line">  )</span><br><span class="line">  spark.createDataFrame(data).toDF(columns: _*)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>We should join it by name as our name value is not stable.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> yearAndPrice = getOsAndYear.join(broadcast(getOsAndPrice), <span class="type">Seq</span>(<span class="string">&quot;id&quot;</span>), <span class="string">&quot;left_outer&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> osAndSize = getOsAndSize.alias(<span class="string">&quot;osAndSize&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> yearPriceAndSize = yearAndPrice.join(osAndSize,</span><br><span class="line">  yearAndPrice(<span class="string">&quot;id&quot;</span>) &lt;=&gt; col(<span class="string">&quot;osAndSize.id&quot;</span>), <span class="string">&quot;left_outer&quot;</span>)</span><br><span class="line">yearPriceAndSize.show(<span class="number">1000</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">+---+-------+----+-------+-----+---+-------+------+</span><br><span class="line">|id |os     |year|os     |price|id |os     |size  |</span><br><span class="line">+---+-------+----+-------+-----+---+-------+------+</span><br><span class="line">|1  |ubuntu |2005|ubuntu |50   |1  |ubuntu |3444  |</span><br><span class="line">|2  |xiaomi |2010|xiaomi |100  |2  |null   |4546  |</span><br><span class="line">|3  |suse   |2015|null   |150  |3  |suse   |5747  |</span><br><span class="line">|4  |samsung|2020|samsung|200  |4  |samsung|687687|</span><br><span class="line">+---+-------+----+-------+-----+---+-------+------+</span><br></pre></td></tr></table></figure>
<p>The weird things happened. Our table has 3 identical <code>os</code> columns.
If we try to select this os column, the exception would be reproduced.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yearPriceAndSize.select(<span class="string">&quot;os&quot;</span>) </span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; org.apache.spark.sql.AnalysisException: Reference &#x27;os&#x27; is ambiguous, could be: default.year_table.os, os, osAndSize.os.;</span><br></pre></td></tr></table></figure>

<h1 id="2-Solution"><a href="#2-Solution" class="headerlink" title="2. Solution"></a>2. Solution</h1><h2 id="2-1-Drop-redundant-columns-before-join"><a href="#2-1-Drop-redundant-columns-before-join" class="headerlink" title="2.1 Drop redundant columns before join"></a>2.1 Drop redundant columns before join</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> yearAndPrice = getOsAndYear</span><br><span class="line">  .join(broadcast(getOsAndPrice.drop(<span class="string">&quot;os&quot;</span>)<span class="comment">/*drop os*/</span>), <span class="type">Seq</span>(<span class="string">&quot;id&quot;</span>), <span class="string">&quot;left_outer&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> osAndSize = getOsAndSize.alias(<span class="string">&quot;osAndSize&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> yearPriceAndSize = yearAndPrice.join(osAndSize.drop(<span class="string">&quot;os&quot;</span>),<span class="comment">// drop os</span></span><br><span class="line">  yearAndPrice(<span class="string">&quot;id&quot;</span>) &lt;=&gt; col(<span class="string">&quot;osAndSize.id&quot;</span>), <span class="string">&quot;left_outer&quot;</span>)</span><br><span class="line">yearPriceAndSize.select(<span class="string">&quot;os&quot;</span>).show(<span class="number">100</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">+-------+</span><br><span class="line">|os     |</span><br><span class="line">+-------+</span><br><span class="line">|ubuntu |</span><br><span class="line">|xiaomi |</span><br><span class="line">|suse   |</span><br><span class="line">|samsung|</span><br><span class="line">+-------+</span><br></pre></td></tr></table></figure>
<p>Of course this solution is so cumbersome due to we should drop the redundant columns again and again.
As far as we know, sometimes we only know some important columns in some tables in our live environment
rather than all the columns. If everytime we remove them, we would repeat ourselves again and again.</p>
<h2 id="2-2-Use-a-relative-way-to-keep-left-redundant-columns-or-right-redundant-columns"><a href="#2-2-Use-a-relative-way-to-keep-left-redundant-columns-or-right-redundant-columns" class="headerlink" title="2.2 Use a relative way to keep left redundant columns or right redundant columns"></a>2.2 Use a relative way to keep left redundant columns or right redundant columns</h2><p>We can use <code>Set</code> to find the columns we want to keep or remove. Usually our join has a main DataFrame, which is a DataFrame
that we only want to add some necessary columns after join. For example, the <code>year_table</code> is the main DataFrame in the
1st join due to we want to add the <code>price</code> column to the <code>year_table</code> rather than add the <code>year</code> to others.</p>
<p>After the joining, we will apply a select, in which we only keeps the columns that comes from a main DataFrame and the
columns that the main DataFrame don’t have. This is what clearRedundant will do.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropRedundantColumnsAfterJoin</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> osAndPrice = broadcast(getOsAndPrice).alias(<span class="string">&quot;osAndPrice&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> osAndYear = getOsAndYear.alias(<span class="string">&quot;osAndYear&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> osAndSize = getOsAndSize.alias(<span class="string">&quot;osAndSize&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> yearAndPrice = osAndYear</span><br><span class="line">    .join(osAndPrice, <span class="type">Seq</span>(<span class="string">&quot;id&quot;</span>), <span class="string">&quot;left_outer&quot;</span>)</span><br><span class="line">    .transform(clearRedundant(osAndYear, <span class="string">&quot;osAndYear&quot;</span>, osAndPrice, <span class="string">&quot;osAndPrice&quot;</span>))</span><br><span class="line">    .alias(<span class="string">&quot;yearAndPrice&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> yearPriceAndSize =</span><br><span class="line">    osAndSize.join(yearAndPrice, <span class="type">Seq</span>(<span class="string">&quot;id&quot;</span>), <span class="string">&quot;right_outer&quot;</span>)</span><br><span class="line">      .transform(clearRedundant(yearAndPrice, <span class="string">&quot;yearAndPrice&quot;</span>, osAndSize, <span class="string">&quot;osAndSize&quot;</span>))</span><br><span class="line">  yearPriceAndSize.show(<span class="number">200</span>, <span class="literal">false</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clearRedundant</span></span>(mainDf: <span class="type">DataFrame</span>, mainDfAlias: <span class="type">String</span>,</span><br><span class="line">                   appendDf: <span class="type">DataFrame</span>, appendDfAlias: <span class="type">String</span>): <span class="type">DataFrame</span> =&gt; <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> mainCols = mainDf.columns.toSet</span><br><span class="line">  <span class="keyword">val</span> appendDfCols = appendDf.columns.toSet</span><br><span class="line">  <span class="comment">// Usually our join is to append some columns to a main DataFrame</span></span><br><span class="line">  <span class="comment">// So the columns to be append is other DataFrame&#x27;s columns - the columns we already have in main DataFrame</span></span><br><span class="line">  <span class="keyword">val</span> appendCols: <span class="type">Set</span>[<span class="type">String</span>] = appendDfCols -- mainCols</span><br><span class="line">  <span class="keyword">val</span> colsAfterJoin: <span class="type">Set</span>[<span class="type">Column</span>] = &#123;</span><br><span class="line">    <span class="comment">// here is to remove the column alias to make sure the new joined df have no redundant alias</span></span><br><span class="line">    mainCols.map(c =&gt; col(<span class="string">s&quot;<span class="subst">$mainDfAlias</span>.<span class="subst">$c</span>&quot;</span>).as(c)) union</span><br><span class="line">      appendCols.map(c =&gt; col(<span class="string">s&quot;<span class="subst">$appendDfAlias</span>.<span class="subst">$c</span>&quot;</span>).as(c))</span><br><span class="line">  &#125;</span><br><span class="line">  finalDf =&gt; finalDf.select(colsAfterJoin.toList: _*)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">+-------+----+---+-----+------+</span><br><span class="line">|os     |year|id |price|size  |</span><br><span class="line">+-------+----+---+-----+------+</span><br><span class="line">|ubuntu |2005|1  |50   |3444  |</span><br><span class="line">|xiaomi |2010|2  |100  |4546  |</span><br><span class="line">|suse   |2015|3  |150  |5747  |</span><br><span class="line">|samsung|2020|4  |200  |687687|</span><br><span class="line">+-------+----+---+-----+------+</span><br></pre></td></tr></table></figure>

<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>The code in this article: <a target="_blank" rel="noopener" href="https://github.com/Solodye/spark-demo/blob/main/src/main/scala/net/i18ndev/spark/alias/ReferenceAmbiguousReproduce.scala">https://github.com/Solodye/spark-demo/blob/main/src/main/scala/net/i18ndev/spark/alias/ReferenceAmbiguousReproduce.scala</a></p>
<p>How to mock Hive in Spark app: <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/55068459/setup-spark-for-ci-how-to-mock-hive-tables">https://stackoverflow.com/questions/55068459/setup-spark-for-ci-how-to-mock-hive-tables</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/02/How%20to%20use%20Window%20in%20Spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YzRuntime's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | YzRuntime's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/06/02/How%20to%20use%20Window%20in%20Spark/" class="post-title-link" itemprop="url">How to use Window in Spark</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-06-02 18:53:54" itemprop="dateCreated datePublished" datetime="2021-06-02T18:53:54+08:00">2021-06-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-07-06 13:57:58" itemprop="dateModified" datetime="2025-07-06T13:57:58+08:00">2025-07-06</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>When we use groupBy, it generates a <strong>single</strong> value from every group.</p>
<p>If we not only want to operate on a group of rows but also want our function return a single value for every input row, we should use window function.</p>
<h1 id="Example-of-last-activity-time"><a href="#Example-of-last-activity-time" class="headerlink" title="Example of last_activity_time"></a>Example of last_activity_time</h1><h2 id="Business-requirement"><a href="#Business-requirement" class="headerlink" title="Business requirement"></a>Business requirement</h2><p>We have a user table with 3 columns. When user click our app, our tracking application will save user’s tracking record. For example: user Tom used its Linux device visited our website at 1st day, then uses ios device visited on 2nd and 3rd days.</p>
<table>
<thead>
<tr>
<th>user_name</th>
<th>active_timestamp</th>
<th>os</th>
</tr>
</thead>
<tbody><tr>
<td>Tom</td>
<td>1</td>
<td>linux</td>
</tr>
<tr>
<td>Tom</td>
<td>2</td>
<td>ios</td>
</tr>
<tr>
<td>Tom</td>
<td>3</td>
<td>ios</td>
</tr>
<tr>
<td>Speike</td>
<td>3</td>
<td>windows</td>
</tr>
<tr>
<td>Speike</td>
<td>4</td>
<td>ios</td>
</tr>
<tr>
<td>Speike</td>
<td>5</td>
<td>ios</td>
</tr>
<tr>
<td>Jerry</td>
<td>6</td>
<td>android</td>
</tr>
<tr>
<td>Jerry</td>
<td>7</td>
<td>ios</td>
</tr>
<tr>
<td>Jerry</td>
<td>8</td>
<td>windows</td>
</tr>
<tr>
<td>Jerry</td>
<td>9</td>
<td>linux</td>
</tr>
<tr>
<td>Jerry</td>
<td>10</td>
<td>macbook</td>
</tr>
</tbody></table>
<p>Our requirement is to calculate the last active timestamp of Tom, Jerry and Speike.</p>
<h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><h3 id="Prepare-for-data"><a href="#Prepare-for-data" class="headerlink" title="Prepare for data"></a>Prepare for data</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepareExampleData</span></span>: <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> columns = <span class="type">Seq</span>(<span class="string">&quot;user_name&quot;</span>, <span class="string">&quot;activity_time&quot;</span>, <span class="string">&quot;os&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> data = <span class="type">Seq</span>(</span><br><span class="line">    (<span class="string">&quot;Tom&quot;</span>, <span class="number">1</span>, <span class="string">&quot;linux&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;Tom&quot;</span>,	<span class="number">2</span>, <span class="string">&quot;ios&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;Tom&quot;</span>, <span class="number">3</span>, <span class="string">&quot;ios&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;Speike&quot;</span>, <span class="number">3</span>, <span class="string">&quot;windows&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;Speike&quot;</span>, <span class="number">4</span>, <span class="string">&quot;ios&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;Speike&quot;</span>, <span class="number">5</span>, <span class="string">&quot;ios&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;Jerry&quot;</span>, <span class="number">6</span>, <span class="string">&quot;android&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;Jerry&quot;</span>, <span class="number">7</span>, <span class="string">&quot;ios&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;Jerry&quot;</span>, <span class="number">8</span>, <span class="string">&quot;windows&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;Jerry&quot;</span>, <span class="number">9</span>, <span class="string">&quot;linux&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;Jerry&quot;</span>, <span class="number">10</span>, <span class="string">&quot;macbook&quot;</span>)</span><br><span class="line">  )</span><br><span class="line">  spark.createDataFrame(data).toDF(columns: _*)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Use-Spark-SQL"><a href="#Use-Spark-SQL" class="headerlink" title="Use Spark SQL"></a>Use Spark SQL</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculateLastActivitySql</span></span>: <span class="type">DataFrame</span> =&gt; <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  df =&gt;</span><br><span class="line">    df.createTempView(<span class="string">&quot;user_table&quot;</span>)</span><br><span class="line">    df.sqlContext.sql(</span><br><span class="line">      <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        |select *,</span></span><br><span class="line"><span class="string">        |first(activity_time) over(partition by user_name order by activity_time desc) as last_activity_time</span></span><br><span class="line"><span class="string">        |from user_table&quot;&quot;&quot;</span>.stripMargin</span><br><span class="line">    )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Use-Spark-DataFrame-API"><a href="#Use-Spark-DataFrame-API" class="headerlink" title="Use Spark DataFrame API"></a>Use Spark DataFrame API</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculateLastActivity</span></span>: <span class="type">DataFrame</span> =&gt; <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  df =&gt;</span><br><span class="line">    <span class="keyword">val</span> window = <span class="type">Window</span>.partitionBy(<span class="string">&quot;user_name&quot;</span>).orderBy(col(<span class="string">&quot;activity_time&quot;</span>).desc)</span><br><span class="line">    df.withColumn(<span class="string">&quot;last_activity_time&quot;</span>, first(<span class="string">&quot;activity_time&quot;</span>).over(window))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">+---------+-------------+-------+------------------+</span><br><span class="line">|user_name|activity_time|     os|last_activity_time|</span><br><span class="line">+---------+-------------+-------+------------------+</span><br><span class="line">|      Tom|            3|    ios|                 3|</span><br><span class="line">|      Tom|            2|    ios|                 3|</span><br><span class="line">|      Tom|            1|  linux|                 3|</span><br><span class="line">|    Jerry|           10|macbook|                10|</span><br><span class="line">|    Jerry|            9|  linux|                10|</span><br><span class="line">|    Jerry|            8|windows|                10|</span><br><span class="line">|    Jerry|            7|    ios|                10|</span><br><span class="line">|    Jerry|            6|android|                10|</span><br><span class="line">|   Speike|            5|    ios|                 5|</span><br><span class="line">|   Speike|            4|    ios|                 5|</span><br><span class="line">|   Speike|            3|windows|                 5|</span><br><span class="line">+---------+-------------+-------+------------------+</span><br></pre></td></tr></table></figure>

<h1 id="Compare-with-groupBy"><a href="#Compare-with-groupBy" class="headerlink" title="Compare with groupBy"></a>Compare with groupBy</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compareWithGroupby</span></span>: <span class="type">DataFrame</span> =&gt; <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  df =&gt;</span><br><span class="line">    <span class="keyword">val</span> result = df.groupBy(<span class="string">&quot;user_name&quot;</span>).agg(</span><br><span class="line">      max(<span class="string">&quot;activity_time&quot;</span>).as(<span class="string">&quot;last_activity_time&quot;</span>)</span><br><span class="line">    )</span><br><span class="line">    df.join(result, usingColumn = <span class="string">&quot;user_name&quot;</span>)</span><br><span class="line">      .select(<span class="string">&quot;user_name&quot;</span>, <span class="string">&quot;activity_time&quot;</span>, <span class="string">&quot;os&quot;</span>, <span class="string">&quot;last_activity_time&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">== <span class="type">Physical</span> <span class="type">Plan</span> ==</span><br><span class="line">*(<span class="number">3</span>) <span class="type">Project</span> [user_name#<span class="number">6</span>, activity_time#<span class="number">7</span>, os#<span class="number">8</span>, last_activity_time#<span class="number">16</span>]</span><br><span class="line">+- *(<span class="number">3</span>) <span class="type">BroadcastHashJoin</span> [user_name#<span class="number">6</span>], [user_name#<span class="number">19</span>], <span class="type">Inner</span>, <span class="type">BuildRight</span></span><br><span class="line">   :- <span class="type">LocalTableScan</span> [user_name#<span class="number">6</span>, activity_time#<span class="number">7</span>, os#<span class="number">8</span>]</span><br><span class="line">   +- <span class="type">BroadcastExchange</span> <span class="type">HashedRelationBroadcastMode</span>(<span class="type">List</span>(input[<span class="number">0</span>, string, <span class="literal">true</span>]))</span><br><span class="line">      +- *(<span class="number">2</span>) <span class="type">HashAggregate</span>(keys=[user_name#<span class="number">19</span>], functions=[max(activity_time#<span class="number">20</span>)])</span><br><span class="line">         +- <span class="type">Exchange</span> hashpartitioning(user_name#<span class="number">19</span>, <span class="number">200</span>)</span><br><span class="line">            +- *(<span class="number">1</span>) <span class="type">HashAggregate</span>(keys=[user_name#<span class="number">19</span>], functions=[partial_max(activity_time#<span class="number">20</span>)])</span><br><span class="line">               +- <span class="type">LocalTableScan</span> [user_name#<span class="number">19</span>, activity_time#<span class="number">20</span>]</span><br><span class="line">== <span class="type">Physical</span> <span class="type">Plan</span> ==</span><br><span class="line"><span class="type">Window</span> [first(activity_time#<span class="number">7</span>, <span class="literal">false</span>) windowspecdefinition(user_name#<span class="number">6</span>, activity_time#<span class="number">7</span> <span class="type">DESC</span> <span class="type">NULLS</span> <span class="type">LAST</span>, specifiedwindowframe(<span class="type">RangeFrame</span>, unboundedpreceding$(), currentrow$())) <span class="type">AS</span> last_activity_time#<span class="number">34</span>], [user_name#<span class="number">6</span>], [activity_time#<span class="number">7</span> <span class="type">DESC</span> <span class="type">NULLS</span> <span class="type">LAST</span>]</span><br><span class="line">+- *(<span class="number">1</span>) <span class="type">Sort</span> [user_name#<span class="number">6</span> <span class="type">ASC</span> <span class="type">NULLS</span> <span class="type">FIRST</span>, activity_time#<span class="number">7</span> <span class="type">DESC</span> <span class="type">NULLS</span> <span class="type">LAST</span>], <span class="literal">false</span>, <span class="number">0</span></span><br><span class="line">   +- <span class="type">Exchange</span> hashpartitioning(user_name#<span class="number">6</span>, <span class="number">200</span>)</span><br><span class="line">      +- <span class="type">LocalTableScan</span> [user_name#<span class="number">6</span>, activity_time#<span class="number">7</span>, os#<span class="number">8</span>]</span><br></pre></td></tr></table></figure>

<h1 id="Conditioned-window-implementation"><a href="#Conditioned-window-implementation" class="headerlink" title="Conditioned window implementation"></a>Conditioned window implementation</h1><p>Now we have another requirement: calculate the last_ios_active_time.</p>
<p>last_ios_active_time represents the last time that user uses its ios platform.</p>
<p>For example, Jerry uses ios to login our website at 7th day while he uses Windows device on 8th day. So Jerry’s last_ios_activity_time is 7.</p>
<table>
<thead>
<tr>
<th>user_name</th>
<th>active_timestamp</th>
<th>os</th>
<th>last_ios_active_time</th>
</tr>
</thead>
<tbody><tr>
<td>Tom</td>
<td>1</td>
<td>linux</td>
<td></td>
</tr>
<tr>
<td>Tom</td>
<td>2</td>
<td>ios</td>
<td></td>
</tr>
<tr>
<td>Tom</td>
<td>3</td>
<td>ios</td>
<td>3</td>
</tr>
<tr>
<td>Speike</td>
<td>3</td>
<td>windows</td>
<td></td>
</tr>
<tr>
<td>Speike</td>
<td>4</td>
<td>ios</td>
<td></td>
</tr>
<tr>
<td>Speike</td>
<td>5</td>
<td>ios</td>
<td>5</td>
</tr>
<tr>
<td>Jerry</td>
<td>6</td>
<td>android</td>
<td></td>
</tr>
<tr>
<td>Jerry</td>
<td>7</td>
<td>ios</td>
<td>7</td>
</tr>
<tr>
<td>Jerry</td>
<td>8</td>
<td>windows</td>
<td></td>
</tr>
<tr>
<td>Jerry</td>
<td>9</td>
<td>linux</td>
<td></td>
</tr>
<tr>
<td>Jerry</td>
<td>10</td>
<td>macbook</td>
<td></td>
</tr>
</tbody></table>
<p>If we want to calculate this metric, we should have a window with a condition: only consider the record that  os = “ios”.</p>
<p>To implement this, we can duplicate a temporary column for window.</p>
<table>
<thead>
<tr>
<th>user_name</th>
<th>active_timestamp</th>
<th>os</th>
<th>_tmp_window_order_timestamp</th>
</tr>
</thead>
<tbody><tr>
<td>Tom</td>
<td>1</td>
<td>linux</td>
<td>null</td>
</tr>
<tr>
<td>Tom</td>
<td>2</td>
<td>ios</td>
<td>2</td>
</tr>
<tr>
<td>Tom</td>
<td>3</td>
<td>ios</td>
<td>3</td>
</tr>
<tr>
<td>Speike</td>
<td>3</td>
<td>windows</td>
<td>null</td>
</tr>
<tr>
<td>Speike</td>
<td>4</td>
<td>ios</td>
<td>4</td>
</tr>
<tr>
<td>Speike</td>
<td>5</td>
<td>ios</td>
<td>5</td>
</tr>
<tr>
<td>Jerry</td>
<td>6</td>
<td>android</td>
<td>null</td>
</tr>
<tr>
<td>Jerry</td>
<td>7</td>
<td>ios</td>
<td>7</td>
</tr>
<tr>
<td>Jerry</td>
<td>8</td>
<td>windows</td>
<td>null</td>
</tr>
<tr>
<td>Jerry</td>
<td>9</td>
<td>linux</td>
<td>null</td>
</tr>
<tr>
<td>Jerry</td>
<td>10</td>
<td>macbook</td>
<td>null</td>
</tr>
</tbody></table>
<p>And then set a window order by this temporary timestamp.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcLastIosActivityTime</span></span>(tmp_col_name: <span class="type">String</span> = <span class="string">&quot;_tmp_window_order_timestamp&quot;</span>)</span><br><span class="line">                            : <span class="type">DataFrame</span> =&gt; <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  df =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> conditionedDf = df.withColumn(tmp_col_name,</span><br><span class="line">      when(col(<span class="string">&quot;os&quot;</span>) === <span class="string">&quot;ios&quot;</span>, col(<span class="string">&quot;activity_time&quot;</span>)).otherwise(<span class="literal">null</span>))</span><br><span class="line">    <span class="keyword">val</span> conditionedWindow =</span><br><span class="line">      <span class="type">Window</span>.partitionBy(<span class="string">&quot;user_name&quot;</span>).orderBy(col(tmp_col_name).desc)</span><br><span class="line">    conditionedDf.withColumn(<span class="string">&quot;last_ios_active_time&quot;</span>, first(tmp_col_name).over(conditionedWindow))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">+---------+-------------+-------+---------------------------+--------------------+</span><br><span class="line">|user_name|activity_time|     os|_tmp_window_order_timestamp|last_ios_active_time|</span><br><span class="line">+---------+-------------+-------+---------------------------+--------------------+</span><br><span class="line">|      Tom|            3|    ios|                          3|                   3|</span><br><span class="line">|      Tom|            2|    ios|                          2|                   3|</span><br><span class="line">|      Tom|            1|  linux|                       null|                   3|</span><br><span class="line">|    Jerry|            7|    ios|                          7|                   7|</span><br><span class="line">|    Jerry|            6|android|                       null|                   7|</span><br><span class="line">|    Jerry|            8|windows|                       null|                   7|</span><br><span class="line">|    Jerry|            9|  linux|                       null|                   7|</span><br><span class="line">|    Jerry|           10|macbook|                       null|                   7|</span><br><span class="line">|   Speike|            5|    ios|                          5|                   5|</span><br><span class="line">|   Speike|            4|    ios|                          4|                   5|</span><br><span class="line">|   Speike|            3|windows|                       null|                   5|</span><br><span class="line">+---------+-------------+-------+---------------------------+--------------------+</span><br></pre></td></tr></table></figure>
<h1 id="Boundary"><a href="#Boundary" class="headerlink" title="Boundary"></a>Boundary</h1><p>Sometimes we may add some redundant orderBy command. It supports to be no effect. 
But actually, in some cases, redundant orderBy may lead the unexpected problem.</p>
<p>Our business requirement is to calculate <code>last_ios_activity_time</code>. We have two methods to calculate this metric.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">showWindowBoundary</span> </span>= &#123;</span><br><span class="line">  <span class="keyword">val</span> df = prepareExampleData</span><br><span class="line">  <span class="keyword">val</span> tmp_col_name: <span class="type">String</span> = <span class="string">&quot;_tmp_window_order_timestamp&quot;</span></span><br><span class="line">  <span class="keyword">val</span> conditionedDf = df.withColumn(tmp_col_name,</span><br><span class="line">    when(col(<span class="string">&quot;os&quot;</span>) === <span class="string">&quot;ios&quot;</span>, col(<span class="string">&quot;activity_time&quot;</span>)).otherwise(<span class="literal">null</span>)).cache()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Method 1</span></span><br><span class="line">  println(<span class="string">&quot;Method 1&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> boundaryWindow =</span><br><span class="line">    <span class="type">Window</span>.partitionBy(<span class="string">&quot;user_name&quot;</span>).orderBy(col(<span class="string">&quot;activity_time&quot;</span>).desc)</span><br><span class="line">  conditionedDf.withColumn(<span class="string">&quot;last_ios_active_time&quot;</span>, max(tmp_col_name).over(boundaryWindow)).show()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Method 2</span></span><br><span class="line">  println(<span class="string">&quot;Method 2&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> userWindow =</span><br><span class="line">    <span class="type">Window</span>.partitionBy(<span class="string">&quot;user_name&quot;</span>)</span><br><span class="line">  conditionedDf.withColumn(<span class="string">&quot;last_ios_active_time&quot;</span>, max(tmp_col_name).over(userWindow)).show()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Method 1 has a redundant <code>orderBy(col(&quot;activity_time&quot;).desc)</code> since we finally use <code>max</code> to calculate the metric. While Method 2 doesn’t have any <code>orderBy</code>.</p>
<p>It seems that the redundant <code>orderBy</code> won’t affect anything. But finally we find it will lead to some record in partition cannot be covered by the Window calculation.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Method 1</span><br><span class="line">+---------+-------------+-------+---------------------------+--------------------+</span><br><span class="line">|user_name|activity_time|     os|_tmp_window_order_timestamp|last_ios_active_time|</span><br><span class="line">+---------+-------------+-------+---------------------------+--------------------+</span><br><span class="line">|      Tom|            3|    ios|                          3|                   3|</span><br><span class="line">|      Tom|            2|    ios|                          2|                   3|</span><br><span class="line">|      Tom|            1|  linux|                       null|                   3|</span><br><span class="line">|    Jerry|           10|macbook|                       null|                null|</span><br><span class="line">|    Jerry|            9|  linux|                       null|                null|</span><br><span class="line">|    Jerry|            8|windows|                       null|                null|</span><br><span class="line">|    Jerry|            7|    ios|                          7|                   7|</span><br><span class="line">|    Jerry|            6|android|                       null|                   7|</span><br><span class="line">|   Speike|            5|    ios|                          5|                   5|</span><br><span class="line">|   Speike|            4|    ios|                          4|                   5|</span><br><span class="line">|   Speike|            3|windows|                       null|                   5|</span><br><span class="line">+---------+-------------+-------+---------------------------+--------------------+</span><br></pre></td></tr></table></figure>
<p>From the data we can see that Jerry uses his ios on 7th day, which is the latest activity_time. </p>
<p>If we order all Jerry’s tracking data by activity time descending, there is 3 records before the record that user_name=Jerry and active_timestamp=7. 
This means if our Window’s spec is from the current row to the first row, the record with activity_time=8 to 10 will not be covered.</p>
<p>Method 2 calculated normal value.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Method 2</span><br><span class="line">+---------+-------------+-------+---------------------------+--------------------+</span><br><span class="line">|user_name|activity_time|     os|_tmp_window_order_timestamp|last_ios_active_time|</span><br><span class="line">+---------+-------------+-------+---------------------------+--------------------+</span><br><span class="line">|      Tom|            1|  linux|                       null|                   3|</span><br><span class="line">|      Tom|            2|    ios|                          2|                   3|</span><br><span class="line">|      Tom|            3|    ios|                          3|                   3|</span><br><span class="line">|    Jerry|            6|android|                       null|                   7|</span><br><span class="line">|    Jerry|            7|    ios|                          7|                   7|</span><br><span class="line">|    Jerry|            8|windows|                       null|                   7|</span><br><span class="line">|    Jerry|            9|  linux|                       null|                   7|</span><br><span class="line">|    Jerry|           10|macbook|                       null|                   7|</span><br><span class="line">|   Speike|            3|windows|                       null|                   5|</span><br><span class="line">|   Speike|            4|    ios|                          4|                   5|</span><br><span class="line">|   Speike|            5|    ios|                          5|                   5|</span><br><span class="line">+---------+-------------+-------+---------------------------+--------------------+</span><br></pre></td></tr></table></figure>

<h2 id="Why-null-still-occurs"><a href="#Why-null-still-occurs" class="headerlink" title="Why null still occurs"></a>Why <code>null</code> still occurs</h2><p>We can see the logical plan. When we use <code>orderBy</code>, the default spec is <code>unboundedpreceding to current row</code>.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">conditionedDf.withColumn(<span class="string">&quot;last_ios_active_time&quot;</span>, max(tmp_col_name).over(boundaryWindow)).explain(<span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">== <span class="type">Parsed</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line"><span class="symbol">&#x27;Project</span> [user_name#<span class="number">6</span>, activity_time#<span class="number">7</span>, os#<span class="number">8</span>, _tmp_window_order_timestamp#<span class="number">22</span>, max(<span class="symbol">&#x27;_tmp_window_order_timestamp</span>) windowspecdefinition(<span class="symbol">&#x27;user_name</span>, <span class="symbol">&#x27;activity_time</span> <span class="type">DESC</span> <span class="type">NULLS</span> <span class="type">LAST</span>, unspecifiedframe$()) <span class="type">AS</span> last_ios_active_time#<span class="number">291</span>]</span><br><span class="line">+- <span class="type">Project</span> [user_name#<span class="number">6</span>, activity_time#<span class="number">7</span>, os#<span class="number">8</span>, <span class="type">CASE</span> <span class="type">WHEN</span> (os#<span class="number">8</span> = ios) <span class="type">THEN</span> activity_time#<span class="number">7</span> <span class="type">ELSE</span> cast(<span class="literal">null</span> as int) <span class="type">END</span> <span class="type">AS</span> _tmp_window_order_timestamp#<span class="number">22</span>]</span><br><span class="line">   +- <span class="type">Project</span> [_1#<span class="number">0</span> <span class="type">AS</span> user_name#<span class="number">6</span>, _2#<span class="number">1</span> <span class="type">AS</span> activity_time#<span class="number">7</span>, _3#<span class="number">2</span> <span class="type">AS</span> os#<span class="number">8</span>]</span><br><span class="line">      +- <span class="type">LocalRelation</span> [_1#<span class="number">0</span>, _2#<span class="number">1</span>, _3#<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">== <span class="type">Analyzed</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line">user_name: string, activity_time: int, os: string, _tmp_window_order_timestamp: int, last_ios_active_time: int</span><br><span class="line"><span class="type">Project</span> [user_name#<span class="number">6</span>, activity_time#<span class="number">7</span>, os#<span class="number">8</span>, _tmp_window_order_timestamp#<span class="number">22</span>, last_ios_active_time#<span class="number">291</span>]</span><br><span class="line">+- <span class="type">Project</span> [user_name#<span class="number">6</span>, activity_time#<span class="number">7</span>, os#<span class="number">8</span>, _tmp_window_order_timestamp#<span class="number">22</span>, last_ios_active_time#<span class="number">291</span>, last_ios_active_time#<span class="number">291</span>]</span><br><span class="line">   +- <span class="type">Window</span> [max(_tmp_window_order_timestamp#<span class="number">22</span>) windowspecdefinition(user_name#<span class="number">6</span>, activity_time#<span class="number">7</span> <span class="type">DESC</span> <span class="type">NULLS</span> <span class="type">LAST</span>, specifiedwindowframe(<span class="type">RangeFrame</span>, unboundedpreceding$(), currentrow$())) <span class="type">AS</span> last_ios_active_time#<span class="number">291</span>], [user_name#<span class="number">6</span>], [activity_time#<span class="number">7</span> <span class="type">DESC</span> <span class="type">NULLS</span> <span class="type">LAST</span>]</span><br><span class="line">      +- <span class="type">Project</span> [user_name#<span class="number">6</span>, activity_time#<span class="number">7</span>, os#<span class="number">8</span>, _tmp_window_order_timestamp#<span class="number">22</span>]</span><br><span class="line">         +- <span class="type">Project</span> [user_name#<span class="number">6</span>, activity_time#<span class="number">7</span>, os#<span class="number">8</span>, <span class="type">CASE</span> <span class="type">WHEN</span> (os#<span class="number">8</span> = ios) <span class="type">THEN</span> activity_time#<span class="number">7</span> <span class="type">ELSE</span> cast(<span class="literal">null</span> as int) <span class="type">END</span> <span class="type">AS</span> _tmp_window_order_timestamp#<span class="number">22</span>]</span><br><span class="line">            +- <span class="type">Project</span> [_1#<span class="number">0</span> <span class="type">AS</span> user_name#<span class="number">6</span>, _2#<span class="number">1</span> <span class="type">AS</span> activity_time#<span class="number">7</span>, _3#<span class="number">2</span> <span class="type">AS</span> os#<span class="number">8</span>]</span><br><span class="line">               +- <span class="type">LocalRelation</span> [_1#<span class="number">0</span>, _2#<span class="number">1</span>, _3#<span class="number">2</span>]</span><br></pre></td></tr></table></figure>

<p>When we don’t use <code>orderBy</code>, the default spec is <code>unboundedproceding, unboundedfollowing</code>.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">conditionedDf.withColumn(<span class="string">&quot;last_ios_active_time&quot;</span>, max(tmp_col_name).over(userWindow)).explain</span><br><span class="line"></span><br><span class="line">== <span class="type">Parsed</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line"><span class="symbol">&#x27;Project</span> [user_name#<span class="number">865</span>, activity_time#<span class="number">866</span>, os#<span class="number">867</span>, _tmp_window_order_timestamp#<span class="number">871</span>, max(<span class="symbol">&#x27;_tmp_window_order_timestamp</span>) windowspecdefinition(<span class="symbol">&#x27;user_name</span>, unspecifiedframe$()) <span class="type">AS</span> last_ios_active_time#<span class="number">1383</span>]</span><br><span class="line">+- <span class="type">Project</span> [user_name#<span class="number">865</span>, activity_time#<span class="number">866</span>, os#<span class="number">867</span>, <span class="type">CASE</span> <span class="type">WHEN</span> (os#<span class="number">867</span> = ios) <span class="type">THEN</span> activity_time#<span class="number">866</span> <span class="type">ELSE</span> cast(<span class="literal">null</span> as int) <span class="type">END</span> <span class="type">AS</span> _tmp_window_order_timestamp#<span class="number">871</span>]</span><br><span class="line">   +- <span class="type">Project</span> [_1#<span class="number">859</span> <span class="type">AS</span> user_name#<span class="number">865</span>, _2#<span class="number">860</span> <span class="type">AS</span> activity_time#<span class="number">866</span>, _3#<span class="number">861</span> <span class="type">AS</span> os#<span class="number">867</span>]</span><br><span class="line">      +- <span class="type">LocalRelation</span> [_1#<span class="number">859</span>, _2#<span class="number">860</span>, _3#<span class="number">861</span>]</span><br><span class="line"></span><br><span class="line">== <span class="type">Analyzed</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line">user_name: string, activity_time: int, os: string, _tmp_window_order_timestamp: int, last_ios_active_time: int</span><br><span class="line"><span class="type">Project</span> [user_name#<span class="number">865</span>, activity_time#<span class="number">866</span>, os#<span class="number">867</span>, _tmp_window_order_timestamp#<span class="number">871</span>, last_ios_active_time#<span class="number">1383</span>]</span><br><span class="line">+- <span class="type">Project</span> [user_name#<span class="number">865</span>, activity_time#<span class="number">866</span>, os#<span class="number">867</span>, _tmp_window_order_timestamp#<span class="number">871</span>, last_ios_active_time#<span class="number">1383</span>, last_ios_active_time#<span class="number">1383</span>]</span><br><span class="line">   +- <span class="type">Window</span> [max(_tmp_window_order_timestamp#<span class="number">871</span>) windowspecdefinition(user_name#<span class="number">865</span>, specifiedwindowframe(<span class="type">RowFrame</span>, unboundedpreceding$(), unboundedfollowing$())) <span class="type">AS</span> last_ios_active_time#<span class="number">1383</span>], [user_name#<span class="number">865</span>]</span><br><span class="line">      +- <span class="type">Project</span> [user_name#<span class="number">865</span>, activity_time#<span class="number">866</span>, os#<span class="number">867</span>, _tmp_window_order_timestamp#<span class="number">871</span>]</span><br><span class="line">         +- <span class="type">Project</span> [user_name#<span class="number">865</span>, activity_time#<span class="number">866</span>, os#<span class="number">867</span>, <span class="type">CASE</span> <span class="type">WHEN</span> (os#<span class="number">867</span> = ios) <span class="type">THEN</span> activity_time#<span class="number">866</span> <span class="type">ELSE</span> cast(<span class="literal">null</span> as int) <span class="type">END</span> <span class="type">AS</span> _tmp_window_order_timestamp#<span class="number">871</span>]</span><br><span class="line">            +- <span class="type">Project</span> [_1#<span class="number">859</span> <span class="type">AS</span> user_name#<span class="number">865</span>, _2#<span class="number">860</span> <span class="type">AS</span> activity_time#<span class="number">866</span>, _3#<span class="number">861</span> <span class="type">AS</span> os#<span class="number">867</span>]</span><br><span class="line">               +- <span class="type">LocalRelation</span> [_1#<span class="number">859</span>, _2#<span class="number">860</span>, _3#<span class="number">861</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>For the record with activity_time=10, its window calculation result is null.</p>
<img src="/2021/06/02/How%20to%20use%20Window%20in%20Spark/window_function_1.png" class="">

<p>In the second record, its window spec is from current line to the first line.</p>
<img src="/2021/06/02/How%20to%20use%20Window%20in%20Spark/window_function_2.png" class="">

<p>When it comes to the record with activity_time=7, the window spec includes _tmp_window_order_timestamp=7. So the <code>max()</code> calculation result is 7.</p>
<img src="/2021/06/02/How%20to%20use%20Window%20in%20Spark/window_function_4.png" class="">

<p>When it comes to the record with activity=6, its windos spec alos includes 7.</p>
<img src="/2021/06/02/How%20to%20use%20Window%20in%20Spark/window_function_5.png" class="">


<h1 id="Reference-docs"><a href="#Reference-docs" class="headerlink" title="Reference docs"></a>Reference docs</h1><p><a target="_blank" rel="noopener" href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a></p>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/40048919/what-is-the-difference-between-rowsbetween-and-rangebetween">https://stackoverflow.com/questions/40048919/what-is-the-difference-between-rowsbetween-and-rangebetween</a></p>
<p><a href="How%20to%20use%20Window%20in%20Spark%20dc83ce2097b54bda8b3bd62feb8805e6/User%20table%206d68e0caa9aa46a0912ee1c6e610e257.csv">User table</a></p>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/30861919/what-is-rows-unbounded-preceding-used-for-in-teradata">Boundary</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>





</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="user"></i>
    </span>
    <span class="author" itemprop="copyrightHolder"></span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
